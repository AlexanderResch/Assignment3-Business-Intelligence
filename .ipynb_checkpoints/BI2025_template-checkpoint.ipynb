{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f7d15c",
   "metadata": {},
   "source": [
    "First, create a new conda environment named BI2025 and install the required packages from requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2329db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda create -n BI2025 python=3.11 -y\n",
    "#!conda activate BI2025\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5122654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY OR COPY THIS CELL!! \n",
    "# Note: The only imports allowed are Python's standard library, pandas, numpy, scipy, matplotlib, seaborn and scikit-learn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import typing\n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "import json\n",
    "from starvers.starvers import TripleStoreEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c891803-9dfd-4be9-9bb3-46ac99ecad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.tzname = ('Europe/Vienna', 'Europe/Vienna')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79408d3",
   "metadata": {},
   "source": [
    "## Graph-based documentation preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b831a95c",
   "metadata": {},
   "source": [
    "**!!!IMPORTANT!!!**\n",
    "\n",
    "Everytime you work on this notebook, enter your student ID in the `executed_by` variable so that the cell executions are accredited to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41a02423",
   "metadata": {},
   "outputs": [],
   "source": [
    "executed_by ='stud-id_12122531'  # Replace the digits after \"id_\" with your own student ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2160a7",
   "metadata": {},
   "source": [
    "Set your group and student IDs. Do this only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16721334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group id for this project\n",
    "group_id = '54'  # Replace the digits with your group id\n",
    "\n",
    "# Students working on this notebook\n",
    "student_a = 'stud-id_12017130'  # Replace the digits after \"id_\" with student A's student ID\n",
    "student_b = 'stud-id_12122531'  # Replace the digits after \"id_\" with student B's student ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb927186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roles. Don't change these values.\n",
    "code_writer_role = 'code_writer'\n",
    "code_executor_role = 'code_executor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e253f6",
   "metadata": {},
   "source": [
    "Setup the starvers API for logging your steps into our server-sided graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4195fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025\"\n",
    "post_endpoint = \"https://starvers.ec.tuwien.ac.at/BI2025/statements\"\n",
    "engine = TripleStoreEngine(get_endpoint, post_endpoint, skip_connection_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043cee91",
   "metadata": {},
   "source": [
    "Use these prefixes in your notebooks. You can extend this dict with your prefixes of additional ontologies that you use in this notebook. Replace 00 with your group id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e6f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {\n",
    "    'xsd': 'http://www.w3.org/2001/XMLSchema#',\n",
    "    'rdfs': 'http://www.w3.org/2000/01/rdf-schema#',\n",
    "    'foaf': 'http://xmlns.com/foaf/0.1/',\n",
    "    'prov': 'http://www.w3.org/ns/prov#',\n",
    "    'sc': 'https://schema.org/',\n",
    "    'cr': 'http://mlcommons.org/croissant/',\n",
    "    'mls': 'http://www.w3.org/ns/mls#',\n",
    "    'mlso': 'http://w3id.org/mlso',\n",
    "    'siu': 'https://si-digital-framework.org/SI/units/',\n",
    "    'siq': 'https://si-digital-framework.org/SI/quantities/',\n",
    "    'qudt': 'http://qudt.org/schema/qudt/',\n",
    "    '': f'https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/',\n",
    "}\n",
    "\n",
    "prefix_header = '\\n'.join([f'PREFIX {k}: <{v}>' for k, v in prefixes.items()]) + '\\n\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970468d",
   "metadata": {},
   "source": [
    "Ontologies to use\n",
    "* Provenance of the experiment process\n",
    "    * PROV-O: \n",
    "        * doc: https://www.w3.org/TR/prov-o/\n",
    "        * serialization: https://www.w3.org/ns/prov-o\n",
    "* Data used and created\n",
    "    * schema.org - Dataset: \n",
    "        * doc: https://schema.org/Dataset\n",
    "        * serialization: https://schema.org/version/latest/schemaorg-current-https.ttl\n",
    "    * Crossaint\n",
    "        * doc: https://docs.mlcommons.org/croissant/docs/croissant-spec.html\n",
    "        * serialization: https://github.com/mlcommons/croissant/blob/main/docs/croissant.ttl\n",
    "* ML experiments performed\n",
    "    * MLSO: \n",
    "        * doc: https://github.com/dtai-kg/MLSO\n",
    "        * doc: https://dtai-kg.github.io/MLSO/#http://w3id.org/\n",
    "        * serialization: https://dtai-kg.github.io/MLSO/ontology.ttl\n",
    "* Measurements, Metrics, Units\n",
    "    * QUDT\n",
    "        * doc:https://qudt.org/\n",
    "        * doc: https://github.com/qudt/qudt-public-repo\n",
    "        * serialization: https://github.com/qudt/qudt-public-repo/blob/main/src/main/rdf/schema/SCHEMA_QUDT.ttl\n",
    "    * SI Digital Framework\n",
    "        * doc: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/docs/README.md\n",
    "        * doc: https://si-digital-framework.org/\n",
    "        * doc: https://si-digital-framework.org/SI\n",
    "        * serialization: https://github.com/TheBIPM/SI_Digital_Framework/blob/main/SI_Reference_Point/TTL/si.ttl\n",
    "    * Quantities and Units\n",
    "        * doc: https://www.omg.org/spec/Commons\n",
    "        * serialization: https://www.omg.org/spec/Commons/QuantitiesAndUnits.ttl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62393d",
   "metadata": {},
   "source": [
    "Use this function to record execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f08ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def now() -> str:\n",
    "    \"\"\"\n",
    "    Returns the current time in ISO 8601 format with UTC timezone in the following format:\n",
    "    YYYY-MM-DDTHH:MM:SS.sssZ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now(datetime.timezone.utc)\n",
    "    timestamp_formated = timestamp.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]  +\"Z\"\n",
    "\n",
    "    return timestamp_formated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a1605",
   "metadata": {},
   "source": [
    "Register yourself in the Knowledge Graph using ProvO. Change the given name, family name and immatriculation number to reflect your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4080a558",
   "metadata": {},
   "outputs": [
    {
     "ename": "EndPointInternalError",
     "evalue": "EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:926\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m     response = \u001b[43murlopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m.returnFormat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:495\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    494\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:604\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:533\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    532\u001b[39m args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    465\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\urllib\\request.py:613\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 500: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mEndPointInternalError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     13\u001b[39m reigstration_triples_b = [\n\u001b[32m     14\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type foaf:Person .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Agent .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_b\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m <http://purl.obolibrary.org/obo/IAO_0000219> \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m12122531\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m^^xsd:string .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     22\u001b[39m ]\n\u001b[32m     24\u001b[39m role_triples = [\n\u001b[32m     25\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_writer_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_executor_role\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rdf:type prov:Role .\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreigstration_triples_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m engine.insert(reigstration_triples_b, prefixes=prefixes)\n\u001b[32m     32\u001b[39m engine.insert(role_triples, prefixes=prefixes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\starvers\\starvers.py:512\u001b[39m, in \u001b[36mTripleStoreEngine.insert\u001b[39m\u001b[34m(self, triples, prefixes, timestamp, chunk_size)\u001b[39m\n\u001b[32m    510\u001b[39m         insert_statement = statement.format(sparql_prefixes, insert_chunk, \u001b[33m\"\u001b[39m\u001b[33mNOW()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m.sparql_post.setQuery(insert_statement)\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparql_post\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mTriples inserted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:960\u001b[39m, in \u001b[36mSPARQLWrapper.query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mQueryResult\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    943\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    944\u001b[39m \u001b[33;03m    Execute the query.\u001b[39;00m\n\u001b[32m    945\u001b[39m \u001b[33;03m    Exceptions can be raised if either the URI is wrong or the HTTP sends back an error (this is also the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m \u001b[33;03m    :rtype: :class:`QueryResult` instance\u001b[39;00m\n\u001b[32m    959\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\SPARQLWrapper\\Wrapper.py:938\u001b[39m, in \u001b[36mSPARQLWrapper._query\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    936\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m URITooLong(e.read())\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m e.code == \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m938\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndPointInternalError(e.read())\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mEndPointInternalError\u001b[39m: EndPointInternalError: The endpoint returned the HTTP status code 500. \n\nResponse:\nb'Entity pool initialization failure'"
     ]
    }
   ],
   "source": [
    "# Ontologies used: foaf, prov, IAO\n",
    "reigstration_triples_a = [\n",
    "f':{student_a} rdf:type foaf:Person .',\n",
    "f':{student_a} rdf:type prov:Agent .',\n",
    "f':{student_a} foaf:givenName \"Alexander\" .',\n",
    "f':{student_a} foaf:familyName \"Resch\" .',\n",
    "f':{student_a} <http://vivoweb.org/ontology/core#identifier> :{student_a} .',\n",
    "f':{student_a} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_a} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_a} <http://purl.obolibrary.org/obo/IAO_0000219> \"12017130\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "reigstration_triples_b = [\n",
    "f':{student_b} rdf:type foaf:Person .',\n",
    "f':{student_b} rdf:type prov:Agent .',\n",
    "f':{student_b} foaf:givenName \"Jakob\" .',\n",
    "f':{student_b} foaf:familyName \"Kimeswenger\" .',\n",
    "f':{student_b} <http://vivoweb.org/ontology/core#identifier> :{student_b} .',\n",
    "f':{student_b} rdf:type <http://purl.obolibrary.org/obo/IAO_0000578> .',\n",
    "f':{student_b} <http://www.w3.org/2000/01/rdf-schema#label> \"Immatriculation number\" .',\n",
    "f':{student_b} <http://purl.obolibrary.org/obo/IAO_0000219> \"12122531\"^^xsd:string .',\n",
    "]\n",
    "\n",
    "role_triples = [\n",
    "    f':{code_writer_role} rdf:type prov:Role .',\n",
    "    f':{code_executor_role} rdf:type prov:Role .',\n",
    "]\n",
    "\n",
    "\n",
    "engine.insert(reigstration_triples_a, prefixes=prefixes)\n",
    "engine.insert(reigstration_triples_b, prefixes=prefixes)\n",
    "engine.insert(role_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c479ed4",
   "metadata": {},
   "source": [
    "**What not do do**\n",
    "\n",
    "Do not use [blank nodes](https://www.w3.org/wiki/BlankNodes).\n",
    "\n",
    "PROV-O uses blank nodes to connect multiple elements with each other.\n",
    "Such blank nodes (such as _:association) should not be used.\n",
    "Instead, assign a fixed node ID such as\n",
    ":5119fcd7-b571-41e0-9464-a37c7be0f574 by generating them outside of the\n",
    "notebook.\n",
    "We suggest that, for each setting where such a blank node is needed to\n",
    "connect multiple elements, you create a unique hash (using uuid.uuid4())\n",
    "and keep this as hard-coded identifier for the blank node. The template\n",
    "notebook contains examples of this. Do *not* use these provided values,\n",
    "as otherwise, your provenance documentations will all be connected via\n",
    "these identifiers!\n",
    "Also, do not generate them dynamically in every cell execution, e.g. by\n",
    "using uuid.uuid4() in a cell. This would generate many new linking nodes\n",
    "for connecting the same elements.\n",
    "Compute one for each node (cell) where you need them and make sure to\n",
    "use the same one on each re-execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_path = os.path.join(\"data\", \"datasets\", \"weather\")\n",
    "cyclists_data_path = os.path.join(\"data\", \"datasets\", \"cyclists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee069d",
   "metadata": {},
   "source": [
    "## Business Understanding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee88389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Business Understanding Phase\n",
    "\n",
    "business_understanding_phase_executor = [\n",
    "f':business_understanding_phase rdf:type prov:Activity .',\n",
    "f':business_understanding_phase rdfs:label \"Business Understanding Phase\" .', ## Phase 1: Business Understanding\n",
    "]\n",
    "engine.insert(business_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc8a3a-708a-4992-a076-038c53338e89",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9bd9643d1e26a8dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "data_src_and_scenario_comment = \"\"\"\n",
    "We decided to use the \"Corporate Credit Rating with Financial Ratios\" dataset available on Kaggle.\n",
    "The dataset contains a set of financial ratios measuring liquidity, leverage and profitability for multiple companies, together with a corporate credit rating label.\n",
    "The scenario is a financial institution that wants to assess corporate credit risk based on financial statement information.\n",
    "The model should assist analysts in assigning credit ratings in decision on creditworthiness.\n",
    "\"\"\"\n",
    "\n",
    "business_objectives_comment = \"\"\"\n",
    "-) Support analysts with consistent credit rating decisions for coporate clients.\n",
    "-) Identify companies with high credit risk early, to reduce credit losses.\n",
    "-) Decrease the effort for manual review for low risk companies.\n",
    "-) At new credit applications, speed up the rating decisions.\n",
    "\"\"\"\n",
    "\n",
    "business_success_criteria_comment = \"\"\"\n",
    "-) Reduce inconsistencies in ratings between analysts by 15 percent.\n",
    "-) Cut down average review time by 20 percent per company.\n",
    "-) Keep the share of wrongly accepted high-risk comanies below 10 percent.\n",
    "-) Avoid blocking of more than 30 percent of low-risk companies.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_goals_comment = \"\"\"\n",
    "-) Train a multi-class classifier that predicts corporate credit ratings from financial ratios.\n",
    "-) Provide calibrated probability scores to rank companies by risk.\n",
    "-) Identify the most important financial ratios driving the rating decisions.\n",
    "-) Support later analysis of class-wise performance for different rating levels.\n",
    "\"\"\"\n",
    "\n",
    "data_mining_success_criteria_comment = \"\"\"\n",
    "-) Achieve a macro-averaged F1 score of at least 0.7 on the test set.\n",
    "-) Achieve at least a recall of 0.75 for the lowest rating classes.\n",
    "-) Keep the difference between validation and test F1 below 0.05.\n",
    "-) Maintain accuracy for all classes rated above 0.60 where sample size is sufficient.\n",
    "\"\"\"\n",
    "\n",
    "ai_risk_aspects_comment = \"\"\"\n",
    "-) Historical ratings can contain human bias, which the model will then learn.\n",
    "-) Financial ratios behave differently across industries, which can disadvantage some sectors.\n",
    "-) Wrong predictions for low-rated companies may result in financial losses.\n",
    "-) Wrong predictions about high-quality companies lead to loss of business and reputational damage.\n",
    "-) Concept drift can occur because economic conditions change over time.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bu_ass_uuid_executor = \"c79bb857-5e80-4d9e-aefb-a4340c7e0caa\" # Generate once       \n",
    "business_understanding_executor = [\n",
    "f':business_understanding rdf:type prov:Activity .',\n",
    "f':business_understanding sc:isPartOf :business_understanding_phase .', # Connect Activity to Parent Business Understanding Phase Activity\n",
    "f':business_understanding prov:qualifiedAssociation :{bu_ass_uuid_executor} .',\n",
    "f':{bu_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{bu_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{bu_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(business_understanding_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "business_understanding_data_executor = [\n",
    "# 1a\n",
    "f':bu_data_source_and_scenario rdf:type prov:Entity .',\n",
    "f':bu_data_source_and_scenario prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_source_and_scenario rdfs:label \"1a Data Source and Scenario\" .',\n",
    "f':bu_data_source_and_scenario rdfs:comment \"\"\"{data_src_and_scenario_comment}\"\"\" .',\n",
    "# 1b\n",
    "f':bu_business_objectives rdf:type prov:Entity .',\n",
    "f':bu_business_objectives prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_objectives rdfs:label \"1b Business Objectives\" .',\n",
    "f':bu_business_objectives rdfs:comment \"\"\"{business_objectives_comment}\"\"\" .',\n",
    "# 1c\n",
    "f':bu_business_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_business_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_business_success_criteria rdfs:label \"1c Business Success Criteria\" .',\n",
    "f':bu_business_success_criteria rdfs:comment \"\"\"{business_success_criteria_comment}\"\"\" .',\n",
    "# 1d\n",
    "f':bu_data_mining_goals rdf:type prov:Entity .',\n",
    "f':bu_data_mining_goals prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_goals rdfs:label \"1d Data Mining Goals\" .',\n",
    "f':bu_data_mining_goals rdfs:comment \"\"\"{data_mining_goals_comment}\"\"\" .',\n",
    "# 1e\n",
    "f':bu_data_mining_success_criteria rdf:type prov:Entity .',\n",
    "f':bu_data_mining_success_criteria prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_data_mining_success_criteria rdfs:label \"1e Data Mining Success Criteria\" .',\n",
    "f':bu_data_mining_success_criteria rdfs:comment \"\"\"{data_mining_success_criteria_comment}\"\"\" .',\n",
    "# 1f\n",
    "f':bu_ai_risk_aspects rdf:type prov:Entity .',\n",
    "f':bu_ai_risk_aspects prov:wasGeneratedBy :business_understanding .',\n",
    "f':bu_ai_risk_aspects rdfs:label \"1f AI risk aspects\" .',\n",
    "f':bu_ai_risk_aspects rdfs:comment \"\"\"{ai_risk_aspects_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(business_understanding_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae9b28",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce717fb",
   "metadata": {},
   "source": [
    "The following pseudo-code & pseudo-documentation may be used as a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Understanding Phase\n",
    "\n",
    "data_understanding_phase_executor = [\n",
    "f':data_understanding_phase rdf:type prov:Activity .',\n",
    "f':data_understanding_phase rdfs:label \"Data Understanding Phase\" .', \n",
    "]\n",
    "engine.insert(data_understanding_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247a9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data_path = os.path.join(\"data\", \"datasets\", \"corporate_credit\")\n",
    "load_credit_data_code_writer = student_a\n",
    "def load_credit_data()-> pd.DataFrame:\n",
    "\n",
    "    ### Load your data\n",
    "    input_file = os.path.join(credit_data_path, 'corporate_credit.csv')\n",
    "    raw_data = pd.read_csv(input_file,  sep=',', header = 0)\n",
    "\n",
    "    raw_data.columns = [\n",
    "        c.strip()\n",
    "         .lower()\n",
    "         .replace(\" \", \"_\")\n",
    "         .replace(\"/\", \"_\")\n",
    "         .replace(\"-\", \"_\")\n",
    "        for c in raw_data.columns\n",
    "    ]\n",
    "    \n",
    "    def create_date_index(dataframe: pd.DataFrame):\n",
    "    # create year, month, and day columns\n",
    "        index_cols = ['year', 'month', 'day']\n",
    "        dataframe['rating_date'] = pd.to_datetime(dataframe['rating_date'], format= \"%Y-%m-%d\", errors=\"coerce\")\n",
    "        dataframe['year'] = dataframe['rating_date'].dt.year\n",
    "        dataframe['month'] = dataframe['rating_date'].dt.month\n",
    "        dataframe['day'] = dataframe['rating_date'].dt.day\n",
    "\n",
    "        dataframe.sort_values(index_cols, ascending = [True for _ in index_cols], inplace = True)\n",
    "        dataframe.set_index(index_cols, inplace = True)\n",
    "        dataframe.index.set_names(index_cols, inplace = True)\n",
    "        return dataframe\n",
    "\n",
    "    loaded_data = raw_data.copy()\n",
    "    loaded_data['day_of_week'] = pd.to_datetime(loaded_data['rating_date']).dt.day_name()\n",
    "    loaded_data = create_date_index(loaded_data)\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "start_time_ld = now()\n",
    "data = load_credit_data()\n",
    "end_time_ld = now()\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "# Now document the raw data and the loaded data using appropriate ontologies.\n",
    "\n",
    "# Always add these triples for every activity to define the executor!\n",
    "ld_ass_uuid_executor = \"13a98d44-10e5-48d5-8133-ae1ee123b720\" # Generate once\n",
    "load_credit_data_executor = [\n",
    "    f':load_credit_data prov:qualifiedAssociation :{ld_ass_uuid_executor} .',\n",
    "    f':{ld_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ld_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(load_credit_data_executor, prefixes=prefixes)\n",
    "\n",
    "ld_ass_uuid_writer = \"d646cc42-2512-4712-ad1f-936d4f723f69\" # Generate once\n",
    "ld_report = \"\"\"\n",
    "Load the corporate credit rating dataset from CSV, clean column names, parse the rating_date field, and create a hierarchical time index (year, month, day) with an additional day_of_week attribute.\n",
    "\"\"\"\n",
    "load_credit_data_activity = [\n",
    "    ':load_credit_data rdf:type prov:Activity .',\n",
    "    ':load_credit_data sc:isPartOf :data_understanding_phase .',\n",
    "    ':load_credit_data rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':load_credit_data rdfs:comment \"\"\"{ld_report}\"\"\" .', \n",
    "    f':load_credit_data prov:startedAtTime \"{start_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_credit_data prov:endedAtTime \"{end_time_ld}\"^^xsd:dateTime .',\n",
    "    f':load_credit_data prov:qualifiedAssociation :{ld_ass_uuid_writer} .',\n",
    "    f':{ld_ass_uuid_writer} prov:agent :{load_credit_data_code_writer} .',\n",
    "    f':{ld_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ld_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    # INPUT of activity\n",
    "    ':load_credit_data prov:used :raw_data .',\n",
    "    ':load_credit_data prov:used :raw_data_path .',\n",
    "    ':raw_data rdf:type prov:Entity .',\n",
    "    ':raw_data_path rdf:type prov:Entity .',\n",
    "    ':raw_data prov:wasDerivedFrom :raw_data_path .',\n",
    "    # OUTPUT of activity\n",
    "    ':data rdf:type prov:Entity .',\n",
    "    ':data prov:wasGeneratedBy :load_credit_data .',\n",
    "    ':data prov:wasDerivedFrom :raw_data .',\n",
    "]\n",
    "engine.insert(load_credit_data_activity, prefixes=prefixes)\n",
    "\n",
    "# Further descibe the raw data using Croissant\n",
    "raw_data_triples = [\n",
    "    ':raw_data rdf:type sc:Dataset .',\n",
    "    ':raw_data sc:name \\'Corporate credit data set\\' .',\n",
    "    ':raw_data sc:description \\'Credit ratings and financial ratios for corporations.\\' .',\n",
    "    # Continue with futher information about the dataset...\n",
    "    ':corporate_credit_csv rdf:type cr:FileObject .',\n",
    "    ':corporate_credit_csv sc:name \\'corporate_credit.csv\\' .',\n",
    "    ':corporate_credit_csv sc:encodingFormat \\'text/csv\\' .',\n",
    "    ':raw_data sc:distribution :corporate_credit_csv .',\n",
    "    # Continue with further information about the distribution...\n",
    "    ':raw_recordset rdf:type cr:RecordSet .',\n",
    "    ':raw_recordset sc:name \\'Table of corporate credit ratings and financial ratios\\' .',\n",
    "    ':raw_recordset cr:source :corporate_credit_csv .',\n",
    "    ':raw_data cr:recordSet :raw_recordset .',\n",
    "    # Continue with further information about the recordset...\n",
    "    \":raw_recordset cr:field :field_rating_date .\",\n",
    "    \":raw_recordset cr:field :field_rating .\",\n",
    "    \":raw_recordset cr:field :field_binary_rating .\",\n",
    "    \":raw_recordset cr:field :field_current_ratio .\",\n",
    "    \":raw_recordset cr:field :field_debt_equity_ratio .\",\n",
    "\n",
    "    #':raw_recordset cr:field :field_date .',\n",
    "    #':raw_recordset cr:field :field_number .',\n",
    "    #':field_date rdf:type cr:Field .',\n",
    "    #':field_date sc:name \\'date\\' .',\n",
    "    #':field_date sc:description \\'This is a date field of...\\' .',\n",
    "    #':field_date cr:dataType xsd:dateTime .',\n",
    "    \n",
    "    # Continue with futher information about the field...\n",
    "    \n",
    "    \":field_rating_date rdf:type cr:Field .\",\n",
    "    \":field_rating_date sc:name \\'rating_date\\' .\",\n",
    "    \":field_rating_date sc:description \\'Date at which the credit rating was assigned\\' .\",\n",
    "    \":field_rating_date cr:dataType xsd:dateTime .\",\n",
    "\n",
    "    \":field_rating rdf:type cr:Field .\",\n",
    "    \":field_rating sc:name \\'rating\\' .\",\n",
    "    \":field_rating sc:description \\'Long term credit rating symbol (for example AAA, BBB-)\\' .\",\n",
    "    \":field_rating cr:dataType xsd:string .\",\n",
    "\n",
    "    \":field_binary_rating rdf:type cr:Field .\",\n",
    "    \":field_binary_rating sc:name \\'binary_rating\\' .\",\n",
    "    \":field_binary_rating sc:description \\'Good vs bad rating indicator\\' .\",\n",
    "    \":field_binary_rating cr:dataType xsd:integer .\",\n",
    "\n",
    "    \":field_current_ratio rdf:type cr:Field .\",\n",
    "    \":field_current_ratio sc:name \\'current_ratio\\' .\",\n",
    "    \":field_current_ratio sc:description \\'Current assets divided by current liabilities\\' .\",\n",
    "    \":field_current_ratio cr:dataType xsd:double .\",\n",
    "\n",
    "    \":field_debt_equity_ratio rdf:type cr:Field .\",\n",
    "    \":field_debt_equity_ratio sc:name \\'debt_equity_ratio\\' .\",\n",
    "    \":field_debt_equity_ratio sc:description \\'Leverage ratio total debt relative to equity\\' .\",\n",
    "    \":field_debt_equity_ratio cr:dataType xsd:double .\",\n",
    "    \n",
    "    #':field_number rdf:type cr:Field .',\n",
    "    #':field_number sc:name \\'number\\' .',\n",
    "    #':field_number sc:description \\'This field describes..\\' .',\n",
    "    #':field_number cr:dataType xsd:integer .',\n",
    "    # Continue with futher fields...\n",
    "]\n",
    "engine.insert(raw_data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also the output of the load activity is a dataset that can be described with Croissant\n",
    "data_triples = [\n",
    "    ':data rdf:type sc:Dataset .',\n",
    "    ':recordset rdf:type cr:RecordSet .',\n",
    "    ':data cr:recordSet :recordset .',\n",
    "\n",
    "    \":recordset cr:field :field_rating_date .\",\n",
    "    \":recordset cr:field :field_rating .\",\n",
    "    \":recordset cr:field :field_binary_rating .\",\n",
    "    \":recordset cr:field :field_current_ratio .\",\n",
    "    \":recordset cr:field :field_debt_equity_ratio .\",\n",
    "    # The loaded data has additional fields\n",
    "\n",
    "    \":recordset cr:field :field_day_of_week .\",\n",
    "    \":field_day_of_week rdf:type cr:Field .\",\n",
    "    \":field_day_of_week sc:name \\'day_of_week\\' .\",\n",
    "    \":field_day_of_week sc:description \\'Day of week derived from rating_date\\' .\",\n",
    "    \":field_day_of_week cr:dataType xsd:string .\",\n",
    "    \n",
    "]\n",
    "engine.insert(data_triples, prefixes=prefixes)\n",
    "\n",
    "# Also add the units to the fields\n",
    "units_triples = [\n",
    "    ':field_current_ratio qudt:unit qudt:UNITLESS .',\n",
    "    ':field_debt_equity_ratio qudt:unit qudt:UNITLESS .',\n",
    "]\n",
    "engine.insert(units_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d09f3-520b-4928-9195-0aee85f07e0c",
   "metadata": {},
   "source": [
    "## 2a) Attribute types, units, semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83557e18-4a56-49db-8548-1cdf3250a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_semantics_code_writer = student_a\n",
    "\n",
    "attr_semantics_comment = \"\"\"\n",
    "Document attribute groups, units, and semantics for the corporate credit dataset.\n",
    "\n",
    "Financial ratios (liquidity, leverage, profitability, efficiency, growth, cash flow) are dimensionless and express relative quantities.\n",
    "\n",
    "Absolute quantities such as total assets, total liabilities, revenues, and cash are measured in USD.\n",
    "\n",
    "rating is an ordered categorical label with classes 0 to 8, where higher values indicate higher credit quality. binary_rating groups low quality (0,1,2) versus all other classes.\n",
    "\n",
    "rating_date records the decision date and defines the time axis for potential concept drift. Derived calendar attributes (year, month, day, day_of_week) support temporal analysis and checks for seasonality.\n",
    "\"\"\"\n",
    "\n",
    "start_time_attr = now()\n",
    "end_time_attr = now()\n",
    "\n",
    "attr_ass_uuid_executor = \"d5f25983-2468-476f-8db5-18b171d21515\"\n",
    "attr_semantics_executor = [\n",
    "    f':document_attribute_semantics prov:qualifiedAssociation :{attr_ass_uuid_executor} .',\n",
    "    f':{attr_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{attr_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{attr_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(attr_semantics_executor, prefixes=prefixes)\n",
    "\n",
    "attr_ass_uuid_writer = \"73062b36-11c2-4846-ac6e-3a6c0cc943e3\"\n",
    "attr_semantics_activity = [\n",
    "    ':document_attribute_semantics rdf:type prov:Activity .',\n",
    "    ':document_attribute_semantics sc:isPartOf :data_understanding_phase .',\n",
    "    ':document_attribute_semantics rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':document_attribute_semantics rdfs:comment \"\"\"{attr_semantics_comment}\"\"\" .',\n",
    "    f':document_attribute_semantics prov:startedAtTime \"{start_time_attr}\"^^xsd:dateTime .',\n",
    "    f':document_attribute_semantics prov:endedAtTime \"{end_time_attr}\"^^xsd:dateTime .',\n",
    "    f':document_attribute_semantics prov:qualifiedAssociation :{attr_ass_uuid_writer} .',\n",
    "    f':{attr_ass_uuid_writer} prov:agent :{attr_semantics_code_writer} .',\n",
    "    f':{attr_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{attr_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':attribute_semantics_report rdf:type prov:Entity .',\n",
    "    ':attribute_semantics_report rdfs:label \"2a Attribute Types, Units, Semantics\" .',\n",
    "    f':attribute_semantics_report rdfs:comment \"\"\"{attr_semantics_comment}\"\"\" .',\n",
    "    ':attribute_semantics_report prov:wasGeneratedBy :document_attribute_semantics .',\n",
    "]\n",
    "engine.insert(attr_semantics_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf928b4-e6b0-4ea5-a7f5-b70057a903fc",
   "metadata": {},
   "source": [
    "## 2b) Structure of dataset (rows, columns, missing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cae15-264e-4587-a512-8ea6be2ad384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_structure(data: pd.DataFrame) -> dict:\n",
    "    summary = {\n",
    "        \"n_rows\": int(data.shape[0]),\n",
    "        \"n_columns\": int(data.shape[1]),\n",
    "        \"columns\": [],\n",
    "    }\n",
    "\n",
    "    for col in data.columns:\n",
    "        col_info = {\n",
    "            \"name\": col,\n",
    "            \"dtype\": str(data[col].dtype),\n",
    "            \"n_missing\": int(data[col].isna().sum()),\n",
    "            \"n_unique\": int(data[col].nunique(dropna=True)),\n",
    "        }\n",
    "        summary[\"columns\"].append(col_info)\n",
    "\n",
    "    return summary\n",
    "\n",
    "start_time_struct = now()\n",
    "structure_report = summarize_structure(data)\n",
    "end_time_struct = now()\n",
    "\n",
    "print(json.dumps(structure_report, indent=2))\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "structure_code_writer = student_a\n",
    "struct_ass_uuid_executor = \"b6153ca6-7b27-4aa1-b8a8-399ec3b6706c\"\n",
    "check_structure_executor = [\n",
    "    f':check_structure prov:qualifiedAssociation :{struct_ass_uuid_executor} .',\n",
    "    f':{struct_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{struct_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{struct_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(check_structure_executor, prefixes=prefixes)\n",
    "\n",
    "struct_ass_uuid_writer = \"845cfade-8aac-47e0-a4c9-b007dca27734\"\n",
    "struct_comment = \"\"\"\n",
    "Summarize basic structure of the corporate credit dataset, including number of rows, columns, data types, missing values, and unique values per column.\n",
    "\"\"\"\n",
    "check_structure_activity = [\n",
    "    ':check_structure rdf:type prov:Activity .',\n",
    "    ':check_structure sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_structure rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_structure rdfs:comment \"\"\"{struct_comment}\"\"\" .',\n",
    "    f':check_structure prov:startedAtTime \"{start_time_struct}\"^^xsd:dateTime .',\n",
    "    f':check_structure prov:endedAtTime \"{end_time_struct}\"^^xsd:dateTime .',\n",
    "    f':check_structure prov:qualifiedAssociation :{struct_ass_uuid_writer} .',\n",
    "    f':{struct_ass_uuid_writer} prov:agent :{structure_code_writer} .',\n",
    "    f':{struct_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{struct_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_structure prov:used :data .',\n",
    "    ':structure_report rdf:type prov:Entity .',\n",
    "    ':structure_report rdfs:label \"Structure Report\" .',\n",
    "    ':structure_report rdfs:comment \"\"\"{json.dumps(structure_report, indent=2)}\"\"\" .',\n",
    "    ':structure_report prov:wasGeneratedBy :check_structure .',\n",
    "]\n",
    "engine.insert(check_structure_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3100ca-625e-4cfb-85f5-e84f5c40d8df",
   "metadata": {},
   "source": [
    "## 2b) Distribution analysis and skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf190abd-7ddd-4f63-b9d4-011f255b62f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_distributions(\n",
    "    data: pd.DataFrame,\n",
    "    numeric_cols,\n",
    "    categorical_cols,\n",
    ") -> dict:\n",
    "    result = {\n",
    "        \"numeric\": {},\n",
    "        \"categorical\": {},\n",
    "    }\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        series = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "        desc = series.describe()\n",
    "        result[\"numeric\"][col] = {\n",
    "            \"count\": float(desc[\"count\"]),\n",
    "            \"min\": float(desc[\"min\"]),\n",
    "            \"max\": float(desc[\"max\"]),\n",
    "            \"mean\": float(desc[\"mean\"]),\n",
    "            \"std\": float(desc[\"std\"]),\n",
    "            \"median\": float(series.median()),\n",
    "            \"skew\": float(series.skew()),\n",
    "        }\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        vc = data[col].value_counts(dropna=False)\n",
    "        top_values = vc.head(5).to_dict()\n",
    "        result[\"categorical\"][col] = {\n",
    "            \"n_unique\": int(vc.shape[0]),\n",
    "            \"top_5\": {str(k): int(v) for k, v in top_values.items()},\n",
    "        }\n",
    "\n",
    "    return result\n",
    "\n",
    "numeric_cols = [\n",
    "    \"current_ratio\",\n",
    "    \"debt_equity_ratio\",\n",
    "    \"gross_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"asset_turnover\",\n",
    "    \"roa___return_on_assets\",\n",
    "    \"roe___return_on_equity\",\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"rating\",\n",
    "    \"binary_rating\",\n",
    "    \"rating_agency\",\n",
    "    \"sector\",\n",
    "]\n",
    "\n",
    "start_time_dist = now()\n",
    "distribution_report = summarize_distributions(data, numeric_cols, categorical_cols)\n",
    "end_time_dist = now()\n",
    "\n",
    "print(json.dumps(distribution_report, indent=2))\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "\n",
    "distribution_code_writer= student_a\n",
    "dist_ass_uuid_executor = \"cac88e6a-97ef-4deb-b476-95e23145e7d4\"\n",
    "check_distribution_executor = [\n",
    "    f':check_distribution prov:qualifiedAssociation :{dist_ass_uuid_executor} .',\n",
    "    f':{dist_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{dist_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{dist_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(check_distribution_executor, prefixes=prefixes)\n",
    "\n",
    "dist_ass_uuid_writer = \"cd9616a1-fa4b-4d0f-b733-87c43bea4442\"\n",
    "dist_comment = \"\"\"\n",
    "Analyze distributions and skewness of key financial ratios and inspect the frequency distribution of ratings, binary_rating, rating_agency, and sector.\n",
    "\"\"\"\n",
    "check_distribution_activity = [\n",
    "    ':check_distribution rdf:type prov:Activity .',\n",
    "    ':check_distribution sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_distribution rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_distribution rdfs:comment \"\"\"{dist_comment}\"\"\" .',\n",
    "    f':check_distribution prov:startedAtTime \"{start_time_dist}\"^^xsd:dateTime .',\n",
    "    f':check_distribution prov:endedAtTime \"{end_time_dist}\"^^xsd:dateTime .',\n",
    "    f':check_distribution prov:qualifiedAssociation :{dist_ass_uuid_writer} .',\n",
    "    f':{dist_ass_uuid_writer} prov:agent :{distribution_code_writer} .',\n",
    "    f':{dist_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dist_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_distribution prov:used :data .',\n",
    "    ':distribution_report rdf:type prov:Entity .',\n",
    "    ':distribution_report rdfs:comment \"Summary of numeric distributions, skewness, and categorical frequency counts for the corporate credit dataset: {json.dumps(distribution_report, indent=2)}\" .',\n",
    "    ':distribution_report prov:wasGeneratedBy :check_distribution .',\n",
    "]\n",
    "engine.insert(check_distribution_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b798d-65d8-42ca-872f-9b4560639def",
   "metadata": {},
   "source": [
    "## 2b) Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91542edd-38e5-4a9e-adf8-509dbff6a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_code_writer = student_a\n",
    "\n",
    "start_time_corr = now()\n",
    "numeric_cols_corr = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = data[numeric_cols_corr].corr()\n",
    "end_time_corr = now()\n",
    "\n",
    "print(\"Correlation matrix for numerical attributes:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "correlation_comment = \"\"\"\n",
    "Compute pairwise Pearson correlations for numerical attributes.\n",
    "Strong absolute correlations above 0.7 or below -0.7 indicate multicollinearity\n",
    "and redundant information. These findings influence feature selection and model design.\n",
    "\"\"\"\n",
    "\n",
    "corr_ass_uuid_executor = \"39026d2f-0d4b-4de5-8869-a013c57b676d\"\n",
    "correlation_executor_triples = [\n",
    "    f':correlation_analysis prov:qualifiedAssociation :{corr_ass_uuid_executor} .',\n",
    "    f':{corr_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{corr_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{corr_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(correlation_executor_triples, prefixes=prefixes)\n",
    "\n",
    "corr_ass_uuid_writer = \"a7679dfe-0b2d-4c46-8f62-e36f273d3f08\"\n",
    "correlation_activity = [\n",
    "    ':correlation_analysis rdf:type prov:Activity .',\n",
    "    ':correlation_analysis sc:isPartOf :data_understanding_phase .',\n",
    "    ':correlation_analysis rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':correlation_analysis rdfs:comment \"\"\"{correlation_comment}\"\"\" .',\n",
    "    f':correlation_analysis prov:startedAtTime \"{start_time_corr}\"^^xsd:dateTime .',\n",
    "    f':correlation_analysis prov:endedAtTime \"{end_time_corr}\"^^xsd:dateTime .',\n",
    "    ':correlation_analysis prov:used :data .',\n",
    "    f':correlation_analysis prov:qualifiedAssociation :{corr_ass_uuid_writer} .',\n",
    "    f':{corr_ass_uuid_writer} prov:agent :{correlation_code_writer} .',\n",
    "    f':{corr_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{corr_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':correlation_report rdf:type prov:Entity .',\n",
    "    ':correlation_report rdfs:label \"Correlation Matrix Report\" .',\n",
    "    f':correlation_report rdfs:comment \"\"\"{correlation_matrix.to_csv()}\"\"\" .',\n",
    "    ':correlation_report prov:wasGeneratedBy :correlation_analysis .',\n",
    "]\n",
    "engine.insert(correlation_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d135b80-80ac-4b8f-8ec3-a7c0f0beb59e",
   "metadata": {},
   "source": [
    "## 2c) Outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0580e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_outliers_code_writer = student_a\n",
    "\n",
    "def check_outliers(data: pd.DataFrame, threshold=3.0, columns=()) -> dict:\n",
    "    results = {}\n",
    "\n",
    "    ### DIRTY HACK\n",
    "    ### REPLACE WITH YOUR ACTUAL OUTLIER CHECKING\n",
    "    tmp = data.copy()\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "\n",
    "    for col in columns:\n",
    "        values = tmp[col].astype(float)\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            results[col] = []\n",
    "            continue\n",
    "\n",
    "        z_scores = (values - mean) / std\n",
    "\n",
    "        mask = np.abs(z_scores) > threshold\n",
    "        outliers = values[mask].index\n",
    "\n",
    "        outlier_info = [\n",
    "            {\n",
    "                'index': int(idx),\n",
    "                'z_score': float(z_scores.loc[idx]),\n",
    "                'value': float(values.loc[idx]), \n",
    "            }\n",
    "            for idx in outliers\n",
    "        ]\n",
    "\n",
    "        results[col] = outlier_info\n",
    "\n",
    "    return results\n",
    "\n",
    "outlier_cols = [\n",
    "    \"current_ratio\",\n",
    "    \"debt_equity_ratio\",\n",
    "    \"gross_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"roa___return_on_assets\",\n",
    "    \"roe___return_on_equity\",\n",
    "]\n",
    "\n",
    "start_time_co = now()\n",
    "outliers_report = check_outliers(data, threshold=3.0, columns = outlier_cols)\n",
    "end_time_co = now()\n",
    "\n",
    "start_time_ho = now()\n",
    "print(outliers_report)\n",
    "end_time_ho = now()\n",
    "\n",
    "outlier_summary = {col: len(lst) for col, lst in outliers_report.items()}\n",
    "print(outlier_summary)\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => in this case a report\n",
    "# 2. activity inspects the outcome and derives decisions => in this case to remove the outliers that were found\n",
    "# 3. activity follows up on the decision by changing the data => will be done in the data preparation phase\n",
    "\n",
    "# 1. Activty: Checking for outliers and creating the report\n",
    "co_ass_uuid_executor = \"95206ddb-90c8-49c4-bfe8-885fcbd78a41\"\n",
    "check_outliers_executor = [\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_executor} .',\n",
    "    f':{co_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{co_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(check_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "co_ass_uuid_writer = \"2715863d-e3af-4688-8a9f-98ed9ee3b616\"\n",
    "co_comment = \"\"\"\n",
    "Identify potential outliers in key financial ratios using a z-score based approach. Ratios with an absolute z-score larger than 3.0 are flagged as outliers.\n",
    "\"\"\"\n",
    "check_outliers_activity = [\n",
    "    ':check_outliers rdf:type prov:Activity .',\n",
    "    ':check_outliers sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_outliers rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_outliers rdfs:comment \"\"\"{co_comment}\"\"\" .', \n",
    "    f':check_outliers prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':check_outliers prov:qualifiedAssociation :{co_ass_uuid_writer} .',\n",
    "    f':{co_ass_uuid_writer} prov:agent :{check_outliers_code_writer} .',\n",
    "    f':{co_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{co_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_outliers prov:used :data .',\n",
    "    ':outlier_report rdf:type prov:Entity .',\n",
    "    f':outlier_report rdfs:comment \"\"\"{json.dumps(outliers_report, indent=2)}\"\"\" .',\n",
    "    ':outlier_report prov:wasGeneratedBy :check_outliers .',\n",
    "]\n",
    "engine.insert(check_outliers_activity, prefixes=prefixes)\n",
    "\n",
    "# 2. Activity: Inspecting the report and taking a decision on what to do\n",
    "ior_ass_uuid_executor = \"21fe4859-e560-4800-82aa-9aa88923794d\"\n",
    "ior_comment = \"\"\"\n",
    "After inspecting the report, the decision is to cap extreme leverage and profitability ratios in the preparation phase instead of dropping rows, to keep as much data as possible.\n",
    "\"\"\"\n",
    "inspect_outlier_report_executor = student_a\n",
    "inspect_outlier_report_activity = [\n",
    "    ':inspect_outlier_report rdf:type prov:Activity .',\n",
    "    ':inspect_outlier_report sc:isPartOf :data_understanding_phase .',\n",
    "    ':inspect_outlier_report rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':inspect_outlier_report rdfs:comment \"\"\"{ior_comment}\"\"\" .', \n",
    "    f':inspect_outlier_report prov:startedAtTime \"{start_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:endedAtTime \"{end_time_co}\"^^xsd:dateTime .',\n",
    "    f':inspect_outlier_report prov:qualifiedAssociation :{ior_ass_uuid_executor} .',\n",
    "    f':{ior_ass_uuid_executor} prov:agent :{inspect_outlier_report_executor} .',\n",
    "    f':{ior_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ior_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "    ':inspect_outlier_report prov:used :outlier_report .',\n",
    "    ':outlier_decision rdf:type prov:Entity .',\n",
    "    f':outlier_decision rdfs:comment \"\"\"Cap extreme ratio values in the data preparation phase instead of removing rows.\"\"\" .',\n",
    "    ':outlier_decision prov:wasGeneratedBy :inspect_outlier_report .',\n",
    "    # ...\n",
    "]\n",
    "engine.insert(inspect_outlier_report_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3d222-753e-4a4c-947c-0205e29a2976",
   "metadata": {},
   "source": [
    "## 2c) Plausibility check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7998f-615c-4d18-898f-76206fd3562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plausibility(data: pd.DataFrame) -> dict:\n",
    "    checks = {}\n",
    "\n",
    "    checks[\"current_ratio_negative\"] = int((data[\"current_ratio\"] < 0).sum())\n",
    "\n",
    "    checks[\"gross_margin_outside_-100_100\"] = int(\n",
    "        ((data[\"gross_margin\"] < -100) | (data[\"gross_margin\"] > 100)).sum()\n",
    "    )\n",
    "    checks[\"operating_margin_outside_-100_100\"] = int(\n",
    "        ((data[\"operating_margin\"] < -100) | (data[\"operating_margin\"] > 100)).sum()\n",
    "    )\n",
    "\n",
    "    checks[\"roa_below_-100\"] = int((data[\"roa___return_on_assets\"] < -100).sum())\n",
    "    checks[\"roa_above_100\"] = int((data[\"roa___return_on_assets\"] > 100).sum())\n",
    "\n",
    "    checks[\"roe_below_-1000\"] = int((data[\"roe___return_on_equity\"] < -1000).sum())\n",
    "    checks[\"roe_above_1000\"] = int((data[\"roe___return_on_equity\"] > 1000).sum())\n",
    "\n",
    "    return checks\n",
    "\n",
    "start_time_pl = now()\n",
    "plausibility_report = check_plausibility(data)\n",
    "end_time_pl = now()\n",
    "\n",
    "print(json.dumps(plausibility_report, indent=2))\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "plausibility_code_writer = student_a\n",
    "pl_ass_uuid_executor = \"eb91b640-5d83-41ee-9b6d-5de8c7b12b9b\"\n",
    "check_plausibility_executor = [\n",
    "    f':check_plausibility prov:qualifiedAssociation :{pl_ass_uuid_executor} .',\n",
    "    f':{pl_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{pl_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{pl_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(check_plausibility_executor, prefixes=prefixes)\n",
    "\n",
    "pl_ass_uuid_writer = \"18de5079-8470-4cc1-8fd9-d0dd44046dca\"\n",
    "pl_comment = \"\"\"\n",
    "Check the plausibility of key financial ratios by counting values with clearly unrealistic ranges, for example extreme negative or positive liquidity and profitability ratios.\n",
    "\"\"\"\n",
    "check_plausibility_activity = [\n",
    "    ':check_plausibility rdf:type prov:Activity .',\n",
    "    ':check_plausibility sc:isPartOf :data_understanding_phase .',\n",
    "    ':check_plausibility rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':check_plausibility rdfs:comment \"\"\"{pl_comment}\"\"\" .',\n",
    "    f':check_plausibility prov:startedAtTime \"{start_time_pl}\"^^xsd:dateTime .',\n",
    "    f':check_plausibility prov:endedAtTime \"{end_time_pl}\"^^xsd:dateTime .',\n",
    "    f':check_plausibility prov:qualifiedAssociation :{pl_ass_uuid_writer} .',\n",
    "    f':{pl_ass_uuid_writer} prov:agent :{plausibility_code_writer} .',\n",
    "    f':{pl_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{pl_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':check_plausibility prov:used :data .',\n",
    "    ':plausibility_report rdf:type prov:Entity .',\n",
    "    ':plausibility_report rdfs:label \"Plausibility Report\" .',\n",
    "    ':plausibility_report rdfs:comment \"Summary of plausibility checks for financial ratios in the corporate credit dataset: {json.dumps(plausibility_report, indent=2)}\" .',\n",
    "    ':plausibility_report prov:wasGeneratedBy :check_plausibility .',\n",
    "]\n",
    "engine.insert(check_plausibility_activity, prefixes=prefixes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab04104-06a7-46f8-96c7-1ab44fe28a92",
   "metadata": {},
   "source": [
    "## 2d) Visual exploration of key variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1828d5-7146-4cba-be61-222b8cb2a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_code_writer = student_a\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time_vis = now()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "if \"current_ratio\" in data.columns:\n",
    "    data[\"current_ratio\"].hist(ax=axs[0, 0], bins=40)\n",
    "    axs[0, 0].set_title(\"Distribution of Current Ratio\")\n",
    "\n",
    "if \"debt_equity_ratio\" in data.columns:\n",
    "    data[\"debt_equity_ratio\"].hist(ax=axs[0, 1], bins=40)\n",
    "    axs[0, 1].set_title(\"Distribution of Debt to Equity Ratio\")\n",
    "\n",
    "if \"roa___return_on_assets\" in data.columns:\n",
    "    data[\"roa___return_on_assets\"].hist(ax=axs[1, 0], bins=40)\n",
    "    axs[1, 0].set_title(\"Distribution of ROA\")\n",
    "\n",
    "if \"rating\" in data.columns:\n",
    "    data[\"rating\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[1, 1])\n",
    "    axs[1, 1].set_title(\"Rating Class Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "end_time_vis = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "visual_comment = \"\"\"\n",
    "Produce histograms for selected financial ratios and a bar chart for rating classes.\n",
    "The plots show skewed distributions, long tails, and strong class imbalance.\n",
    "These observations support later decisions on outlier handling, transformation,\n",
    "and class weighting strategies.\n",
    "\"\"\"\n",
    "\n",
    "vis_ass_uuid_executor = \"bbeb7ab9-c9f1-4b99-a248-212b9bfc7ae7\"\n",
    "visual_executor_triples = [\n",
    "    f':visual_exploration prov:qualifiedAssociation :{vis_ass_uuid_executor} .',\n",
    "    f':{vis_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{vis_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(visual_executor_triples, prefixes=prefixes)\n",
    "\n",
    "vis_ass_uuid_writer = \"64c6a0ef-5ce3-4ad0-9efb-342b5a03c900\"\n",
    "visual_activity = [\n",
    "    ':visual_exploration rdf:type prov:Activity .',\n",
    "    ':visual_exploration sc:isPartOf :data_understanding_phase .',\n",
    "    ':visual_exploration rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':visual_exploration rdfs:comment \"\"\"{visual_comment}\"\"\" .',\n",
    "    f':visual_exploration prov:startedAtTime \"{start_time_vis}\"^^xsd:dateTime .',\n",
    "    f':visual_exploration prov:endedAtTime \"{end_time_vis}\"^^xsd:dateTime .',\n",
    "    ':visual_exploration prov:used :data .',\n",
    "    f':visual_exploration prov:qualifiedAssociation :{vis_ass_uuid_writer} .',\n",
    "    f':{vis_ass_uuid_writer} prov:agent :{visual_code_writer} .',\n",
    "    f':{vis_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{vis_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':visual_exploration_report rdf:type prov:Entity .',\n",
    "    ':visual_exploration_report rdfs:label \"Visual Exploration Summary\" .',\n",
    "    ':visual_exploration_report rdfs:comment \"\"\"Histograms for key ratios and rating distribution plotted for data understanding.\"\"\" .',\n",
    "    ':visual_exploration_report prov:wasGeneratedBy :visual_exploration .',\n",
    "]\n",
    "engine.insert(visual_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a2fe9-c2ec-4613-be2e-e6ddd83fa500",
   "metadata": {},
   "source": [
    "## 2e) Sensitive attributes and minority groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ea5b9-b672-4ec1-a19a-1a061f2e080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitive_code_writer = student_a\n",
    "\n",
    "sensitive_comment = \"\"\"\n",
    "The dataset does not contain explicit sensitive attributes such as gender or ethnicity. Minority groups appear in rare target classes, especially rating classes 0 and 1. These classes have few observations and need attention in evaluation and model design, for example through class weights or sampling strategies.\n",
    "\"\"\"\n",
    "\n",
    "start_time_sens = now()\n",
    "end_time_sens = now()\n",
    "\n",
    "sensitive_ass_uuid_executor = \"eee58b3c-59ac-4f30-be56-e51564c13b29\"\n",
    "sensitive_executor_triples = [\n",
    "    f':document_sensitive_attributes prov:qualifiedAssociation :{sensitive_ass_uuid_executor} .',\n",
    "    f':{sensitive_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{sensitive_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{sensitive_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(sensitive_executor_triples, prefixes=prefixes)\n",
    "\n",
    "sensitive_ass_uuid_writer = \"04896b6f-8691-498d-ab9d-08a0a5248604\"\n",
    "sensitive_activity = [\n",
    "    ':document_sensitive_attributes rdf:type prov:Activity .',\n",
    "    ':document_sensitive_attributes sc:isPartOf :data_understanding_phase .',\n",
    "    ':document_sensitive_attributes rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':document_sensitive_attributes rdfs:comment \"\"\"{sensitive_comment}\"\"\" .',\n",
    "    f':document_sensitive_attributes prov:startedAtTime \"{start_time_sens}\"^^xsd:dateTime .',\n",
    "    f':document_sensitive_attributes prov:endedAtTime \"{end_time_sens}\"^^xsd:dateTime .',\n",
    "    f':document_sensitive_attributes prov:qualifiedAssociation :{sensitive_ass_uuid_writer} .',\n",
    "    f':{sensitive_ass_uuid_writer} prov:agent :{sensitive_code_writer} .',\n",
    "    f':{sensitive_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{sensitive_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':sensitive_attributes_report rdf:type prov:Entity .',\n",
    "    ':sensitive_attributes_report rdfs:label \"2e Sensitive Attributes and Minority Groups\" .',\n",
    "    f':sensitive_attributes_report rdfs:comment \"\"\"{sensitive_comment}\"\"\" .',\n",
    "    ':sensitive_attributes_report prov:wasGeneratedBy :document_sensitive_attributes .',\n",
    "]\n",
    "engine.insert(sensitive_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99410f4-9dd3-4dc1-be6b-f0724d2a023b",
   "metadata": {},
   "source": [
    "## 2f) Risks and bias in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420261f-2488-4715-8448-5628753148a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_code_writer = student_a\n",
    "\n",
    "bias_comment = \"\"\"\n",
    "Historical ratings reflect past human judgment and internal policies. This creates systematic bias in the target labels. Missing sector information hides structural differences between industries and firm sizes.\n",
    "\n",
    "rating_date spans multiple years, so changes in the economic cycle and internal rating guidelines influence label stability and calibration.\n",
    "\n",
    "Open questions for a domain expert:\n",
    "1) Did rating guidelines change during the covered period.\n",
    "2) Are some industries overrepresented in the dataset.\n",
    "3) Are financial ratios comparable across firm sizes for all rating classes.\n",
    "\"\"\"\n",
    "\n",
    "start_time_bias = now()\n",
    "end_time_bias = now()\n",
    "\n",
    "bias_ass_uuid_executor = \"46aa277a-34f3-4eba-86b3-f167be971cad\"\n",
    "bias_executor_triples = [\n",
    "    f':document_bias_risks prov:qualifiedAssociation :{bias_ass_uuid_executor} .',\n",
    "    f':{bias_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{bias_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{bias_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(bias_executor_triples, prefixes=prefixes)\n",
    "\n",
    "bias_ass_uuid_writer = \"881a1f63-3710-48a8-ac0a-61a5f9bf5869\"\n",
    "bias_activity = [\n",
    "    ':document_bias_risks rdf:type prov:Activity .',\n",
    "    ':document_bias_risks sc:isPartOf :data_understanding_phase .',\n",
    "    ':document_bias_risks rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':document_bias_risks rdfs:comment \"\"\"{bias_comment}\"\"\" .',\n",
    "    f':document_bias_risks prov:startedAtTime \"{start_time_bias}\"^^xsd:dateTime .',\n",
    "    f':document_bias_risks prov:endedAtTime \"{end_time_bias}\"^^xsd:dateTime .',\n",
    "    f':document_bias_risks prov:qualifiedAssociation :{bias_ass_uuid_writer} .',\n",
    "    f':{bias_ass_uuid_writer} prov:agent :{bias_code_writer} .',\n",
    "    f':{bias_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{bias_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':bias_risks_report rdf:type prov:Entity .',\n",
    "    ':bias_risks_report rdfs:label \"2f Risks and Bias in Data\" .',\n",
    "    f':bias_risks_report rdfs:comment \"\"\"{bias_comment}\"\"\" .',\n",
    "    ':bias_risks_report prov:wasGeneratedBy :document_bias_risks .',\n",
    "]\n",
    "engine.insert(bias_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fc54e5-b8ce-47fc-92ea-d51d94367849",
   "metadata": {},
   "source": [
    "## 2g) Planned actions for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743a9ad-610f-47a4-b9ae-c0c603081b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_code_writer = student_a\n",
    "\n",
    "prep_comment = \"\"\"\n",
    "Planned preparation actions based on the data understanding findings:\n",
    "\n",
    "1) Cap extreme outliers in leverage, liquidity, and profitability ratios instead of dropping rows, to keep more observations.\n",
    "2) Apply suitable strategies for missing values, for example dropping rows with many missing entries and imputing single missing ratios.\n",
    "3) Encode rating as ordinal target and keep binary_rating as helper target for alternative evaluation.\n",
    "4) Address class imbalance with stratified splits and class weights during model training.\n",
    "5) Normalize or transform strongly skewed ratios, for example with log transforms for strictly positive variables.\n",
    "\"\"\"\n",
    "\n",
    "start_time_prep = now()\n",
    "end_time_prep = now()\n",
    "\n",
    "prep_ass_uuid_executor = \"beeb3651-2154-48d7-b616-1f040c8deb56\"\n",
    "prep_executor_triples = [\n",
    "    f':document_preparation_actions prov:qualifiedAssociation :{prep_ass_uuid_executor} .',\n",
    "    f':{prep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{prep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{prep_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(prep_executor_triples, prefixes=prefixes)\n",
    "\n",
    "prep_ass_uuid_writer = \"bfc079c5-0dbe-477c-b1cd-56f85f8dcbf8\"\n",
    "prep_activity = [\n",
    "    ':document_preparation_actions rdf:type prov:Activity .',\n",
    "    ':document_preparation_actions sc:isPartOf :data_understanding_phase .',\n",
    "    ':document_preparation_actions rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':document_preparation_actions rdfs:comment \"\"\"{prep_comment}\"\"\" .',\n",
    "    f':document_preparation_actions prov:startedAtTime \"{start_time_prep}\"^^xsd:dateTime .',\n",
    "    f':document_preparation_actions prov:endedAtTime \"{end_time_prep}\"^^xsd:dateTime .',\n",
    "    f':document_preparation_actions prov:qualifiedAssociation :{prep_ass_uuid_writer} .',\n",
    "    f':{prep_ass_uuid_writer} prov:agent :{prep_code_writer} .',\n",
    "    f':{prep_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{prep_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':preparation_actions_report rdf:type prov:Entity .',\n",
    "    ':preparation_actions_report rdfs:label \"2g Planned Data Preparation Actions\" .',\n",
    "    f':preparation_actions_report rdfs:comment \"\"\"{prep_comment}\"\"\" .',\n",
    "    ':preparation_actions_report prov:wasGeneratedBy :document_preparation_actions .',\n",
    "]\n",
    "engine.insert(prep_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16349e3",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d290a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Data Preparation Phase\n",
    "\n",
    "data_preparation_phase_executor = [\n",
    "f':data_preparation_phase rdf:type prov:Activity .',\n",
    "f':data_preparation_phase rdfs:label \"Data Preparation Phase\" .', \n",
    "]\n",
    "engine.insert(data_preparation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d076f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_outliers_code_writer = student_b\n",
    "\n",
    "def handle_outliers(df:pd.DataFrame, outliers_report: dict) -> pd.DataFrame:\n",
    "    threshold = 3.0 # three-sigma rule\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    for col, outliers in outliers_report.items():\n",
    "        if col not in df_clean.columns or len(outliers) == 0:\n",
    "            continue\n",
    "\n",
    "        values = df_clean[col].astype(float)\n",
    "\n",
    "        mean = values.mean()\n",
    "        std = values.std()\n",
    "\n",
    "        if std == 0 or np.isnan(std):\n",
    "            continue\n",
    "\n",
    "        upper_cap = mean + threshold * std\n",
    "        lower_cap = mean - threshold * std\n",
    "\n",
    "        for o in outliers:\n",
    "            pos = o[\"index\"]  # positional index\n",
    "\n",
    "            val = df_clean.iloc[pos, df_clean.columns.get_loc(col)]\n",
    "\n",
    "            if val > upper_cap:\n",
    "                df_clean.iloc[pos, df_clean.columns.get_loc(col)] = upper_cap\n",
    "            elif val < lower_cap:\n",
    "                df_clean.iloc[pos, df_clean.columns.get_loc(col)] = lower_cap\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "start_time_td = now()\n",
    "cleaned_data = handle_outliers(data, outliers_report)\n",
    "end_time_td = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "# This is the continuation of the example from the Data Understanding phase above.\n",
    "# There are three steps involved in this process:\n",
    "# 1. activity creates a figure, report etc. => already done in data understanding phase\n",
    "# 2. activity inspects the outcome and derives decisions => already done in data understanding phase\n",
    "# 3. activity follows up on the decision by changing the data => in this case by removing the the outliers that were found\n",
    "\n",
    "ro_ass_uuid_executor = \"ec7e81e1-86ea-475a-a8d4-c7d8ee535488\"\n",
    "handle_outliers_executor = [\n",
    "    f':handle_outliers prov:qualifiedAssociation :{ro_ass_uuid_executor} .',\n",
    "    f':{ro_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ro_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ro_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_outliers_executor, prefixes=prefixes)\n",
    "\n",
    "td_ass_uuid_writer = \"1405f15a-3545-4014-a962-637f3c10a137\"\n",
    "td_comment = \"\"\"\n",
    "During the outlier handling phase, extreme values that were identified in the Data Understanding phase \n",
    "were addressed in order to reduce their influence on the analysis. Instead of removing entire observations, \n",
    "these outliers were treated so that more data points could be retained while still improving the overall \n",
    "quality and stability of the dataset.\n",
    "\"\"\"\n",
    "handle_outliers_activity = [\n",
    "    ':handle_outliers rdf:type prov:Activity .',\n",
    "    ':handle_outliers sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_outliers rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_outliers rdfs:comment \"\"\"{td_comment}\"\"\" .', \n",
    "    f':handle_outliers prov:startedAtTime \"{start_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:endedAtTime \"{end_time_td}\"^^xsd:dateTime .',\n",
    "    f':handle_outliers prov:qualifiedAssociation :{td_ass_uuid_writer} .',\n",
    "    f':{td_ass_uuid_writer} prov:agent :{handle_outliers_code_writer} .',\n",
    "    f':{td_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{td_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_outliers prov:used :data .',\n",
    "    ':handle_outliers prov:used :outlier_decision .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_outliers .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_outliers_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cff7-8fd5-4ba1-8913-b4f1ccdfda35",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8800ce26b8f3e2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Continue with other tasks of the Data Preparation phase such as binning, scaling etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20b8e8-7d7f-4df5-ba38-62704f020c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_missing_values_code_writer = student_b\n",
    "def handle_missing_values(\n",
    "    df: pd.DataFrame,\n",
    "    ratio_columns: list,\n",
    "    row_missing_threshold: float = 0.4,\n",
    "    col_missing_threshold: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles missing values by:\n",
    "    1) Dropping rows with too many missing values\n",
    "    2) Imputing single missing ratio values using the median\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Drop rows with excessive missing values\n",
    "    # --------------------------------------------------\n",
    "    row_missing_share = df_clean.isna().mean(axis=1)\n",
    "    df_clean = df_clean.loc[row_missing_share <= row_missing_threshold].copy()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Impute ratio columns with limited missingness\n",
    "    # --------------------------------------------------\n",
    "    for col in ratio_columns:\n",
    "        if col not in df_clean.columns:\n",
    "            continue\n",
    "\n",
    "        missing_share = df_clean[col].isna().mean()\n",
    "\n",
    "        # Skip columns with too many missing values\n",
    "        if missing_share == 0 or missing_share > col_missing_threshold:\n",
    "            continue\n",
    "\n",
    "        median_value = df_clean[col].median()\n",
    "\n",
    "        if not np.isnan(median_value):\n",
    "            df_clean[col] = df_clean[col].fillna(median_value)\n",
    "\n",
    "    return df_clean\n",
    "# colums that are finacial ratios and should be filled by mean if empty    \n",
    "ratio_columns = [\n",
    "    \"Current Ratio\",\n",
    "    \"Long-term Debt / Capital\",\n",
    "    \"Debt/Equity Ratio\",\n",
    "    \"Gross Margin\",\n",
    "    \"Operating Margin\",\n",
    "    \"EBIT Margin\",\n",
    "    \"EBITDA Margin\",\n",
    "    \"Pre-Tax Profit Margin\",\n",
    "    \"Net Profit Margin\",\n",
    "    \"Asset Turnover\",\n",
    "    \"ROE - Return On Equity\",\n",
    "    \"Return On Tangible Equity\",\n",
    "    \"ROA - Return On Assets\",\n",
    "    \"ROI - Return On Investment\",\n",
    "    \"Operating Cash Flow Per Share\",\n",
    "    \"Free Cash Flow Per Share\",\n",
    "]\n",
    "start_time_mv = now()\n",
    "cleaned_data = handle_missing_values(cleaned_data, ratio_columns)\n",
    "end_time_mv = now()\n",
    "\n",
    "mv_ass_uuid_executor = \"ec7e81e1-86ea-475a-b9z5-c7d8ee535488\"\n",
    "handle_missing_values_executor = [\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_executor} .',\n",
    "    f':{mv_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{mv_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(handle_missing_values_executor, prefixes=prefixes)\n",
    "\n",
    "mv_ass_uuid_writer = \"1405f15a-3545-6789-a962-637b3c10a137\"\n",
    "mv_comment = \"\"\"\n",
    "In the missing value handling step, observations with a high proportion of missing data were removed to ensure data reliability, \n",
    "while isolated missing values in financial ratio variables were imputed using robust column medians. Non-ratio variables were \n",
    "excluded from imputation.\n",
    "\"\"\"\n",
    "handle_missing_values_activity = [\n",
    "    ':handle_missing_values rdf:type prov:Activity .',\n",
    "    ':handle_missing_values sc:isPartOf :data_preparation_phase .',\n",
    "    ':handle_missing_values rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':handle_missing_values rdfs:comment \"\"\"{mv_comment}\"\"\" .', \n",
    "    f':handle_missing_values prov:startedAtTime \"{start_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:endedAtTime \"{end_time_mv}\"^^xsd:dateTime .',\n",
    "    f':handle_missing_values prov:qualifiedAssociation :{mv_ass_uuid_writer} .',\n",
    "    f':{mv_ass_uuid_writer} prov:agent :{handle_missing_values_code_writer} .',\n",
    "    f':{mv_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{mv_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':handle_missing_values prov:used :data .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :handle_missing_values .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(handle_missing_values_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447e864-ca19-41de-b61a-e2e73863ad2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#3) Encode rating as ordinal target.\n",
    "encode_rating_code_writer = student_b\n",
    "\n",
    "def encode_rating(\n",
    "    df: pd.DataFrame,\n",
    "    rating_col: str = \"rating\",\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    rating_order = [\n",
    "        \"AAA\",\n",
    "        \"AA+\", \"AA\", \"AA-\",\n",
    "        \"A+\", \"A\", \"A-\",\n",
    "        \"BBB+\", \"BBB\", \"BBB-\",\n",
    "        \"BB+\", \"BB\", \"BB-\",\n",
    "        \"B+\", \"B\", \"B-\",\n",
    "        \"CCC+\", \"CCC\", \"CCC-\",\n",
    "        \"D\"\n",
    "    ]\n",
    "\n",
    "    rating_map = {rating: i + 1 for i, rating in enumerate(rating_order)}\n",
    "\n",
    "    df_encoded[rating_col] = df_encoded[rating_col].map(rating_map)\n",
    "\n",
    "    return df_encoded\n",
    "    \n",
    "start_time_er = now()\n",
    "cleaned_data = encode_rating(cleaned_data)\n",
    "end_time_er = now()\n",
    "\n",
    "er_ass_uuid_executor = \"ec7e81e1-86ea-123b-b9z5-c7d8ee535488\"\n",
    "encode_rating_executor = [\n",
    "    f':encode_rating prov:qualifiedAssociation :{er_ass_uuid_executor} .',\n",
    "    f':{er_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{er_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{er_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(encode_rating_executor, prefixes=prefixes)\n",
    "\n",
    "er_ass_uuid_writer = \"1405f15a-1234-6789-a963-637b3c10a137\"\n",
    "er_comment = \"\"\"\n",
    "In this step, the credit rating variable was transformed into an ordinal numeric scale to reflect the natural ordering \n",
    "of credit quality from higher to lower ratings. This makes the rating variable easier to use in the modeling process, \n",
    "while still preserving the relative differences between rating categories. In addition, the binary rating variable was \n",
    "kept unchanged and used as a supporting target for alternative evaluation and comparison of model results.\n",
    "\"\"\"\n",
    "encode_rating_activity = [\n",
    "    ':encode_rating rdf:type prov:Activity .',\n",
    "    ':encode_rating sc:isPartOf :data_preparation_phase .',\n",
    "    ':encode_rating rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':encode_rating rdfs:comment \"\"\"{er_comment}\"\"\" .', \n",
    "    f':encode_rating prov:startedAtTime \"{start_time_er}\"^^xsd:dateTime .',\n",
    "    f':encode_rating prov:endedAtTime \"{end_time_er}\"^^xsd:dateTime .',\n",
    "    f':encode_rating prov:qualifiedAssociation :{er_ass_uuid_writer} .',\n",
    "    f':{er_ass_uuid_writer} prov:agent :{encode_rating_code_writer} .',\n",
    "    f':{er_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{er_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':encode_rating prov:used :data .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :encode_rating .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(encode_rating_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29f254-9bf0-4f53-98e3-418b2e207807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scale_features_code_writer = student_b\n",
    "def scale_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature_columns: list\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[feature_columns] = scaler.fit_transform(\n",
    "        df_scaled[feature_columns]\n",
    "    )\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "scale_columns = [\n",
    "    \"current_ratio\",\n",
    "    \"debt_equity_ratio\",\n",
    "    \"long_term_debt___capital\",\n",
    "    \"gross_margin\",\n",
    "    \"operating_margin\",\n",
    "    \"ebit_margin\",\n",
    "    \"ebitda_margin\",\n",
    "    \"pre_tax_profit_margin\",\n",
    "    \"net_profit_margin\",\n",
    "    \"asset_turnover\",\n",
    "    \"roa___return_on_assets\",\n",
    "    \"roe___return_on_equity\",\n",
    "    \"return_on_tangible_equity\",\n",
    "    \"roi___return_on_investment\",\n",
    "    \"operating_cash_flow_per_share\",\n",
    "    \"free_cash_flow_per_share\",\n",
    "]\n",
    "\n",
    "start_time_sf = now()\n",
    "cleaned_data = scale_features(cleaned_data, scale_columns)\n",
    "end_time_sf = now()\n",
    "\n",
    "sf_ass_uuid_executor = \"ec7e81e1-86ea-345z-b9z5-c7d8ab785488\"\n",
    "scale_features_executor = [\n",
    "    f':scale_features prov:qualifiedAssociation :{sf_ass_uuid_executor} .',\n",
    "    f':{sf_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{sf_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{sf_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(scale_features_executor, prefixes=prefixes)\n",
    "\n",
    "sf_ass_uuid_writer = \"1405f15a-5678-1111-a963-637a3z10a167\"\n",
    "sf_comment = \"\"\"\n",
    "The numerical ratio variables were scaled so that they are on a similar scale and no single variable dominates the model.\n",
    "\"\"\"\n",
    "scale_features_activity = [\n",
    "    ':scale_features rdf:type prov:Activity .',\n",
    "    ':scale_features sc:isPartOf :data_preparation_phase .',\n",
    "    ':scale_features rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':scale_features rdfs:comment \"\"\"{sf_comment}\"\"\" .', \n",
    "    f':scale_features prov:startedAtTime \"{start_time_sf}\"^^xsd:dateTime .',\n",
    "    f':scale_features prov:endedAtTime \"{end_time_sf}\"^^xsd:dateTime .',\n",
    "    f':scale_features prov:qualifiedAssociation :{sf_ass_uuid_writer} .',\n",
    "    f':{sf_ass_uuid_writer} prov:agent :{scale_features_code_writer} .',\n",
    "    f':{sf_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{sf_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':scale_features prov:used :data .',\n",
    "    ':cleaned_data rdf:type prov:Entity .',\n",
    "    ':cleaned_data prov:wasGeneratedBy :encode_rating .',\n",
    "    ':cleaned_data prov:wasDerivedFrom :data .',\n",
    "]\n",
    "engine.insert(scale_features_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbad23-e083-498b-9548-f4cc0db3099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used_code_writer = student_b\n",
    "\n",
    "start_time_nd = now()\n",
    "nd_comment = \"\"\"\n",
    "Binning was not applied to the financial ratio variables. Although binning can simplify continuous variables \n",
    "by grouping them into categories, it also leads to a loss of information. In this dataset, the ratios contain \n",
    "meaningful continuous variation that is important for modeling credit risk. Since outliers were already handled \n",
    "and the models used can work well with continuous variables, keeping the original ratio values was considered \n",
    "more appropriate than applying binning.\n",
    "\n",
    "Additional transformations of numerical variables, such as logarithmic or power transformations, were considered to \n",
    "address skewed distributions. These transformations were not applied because several financial ratios contain zero or \n",
    "negative values, and applying a uniform transformation could distort the original economic meaning of the variables. \n",
    "After handling outliers, the remaining distributions were considered acceptable for further analysis.\n",
    "\n",
    "Feature scaling was considered to bring numerical variables onto a comparable scale. This step was not applied at this \n",
    "stage, as the decision depends on the final choice of modeling approach. Scaling can be applied later if required by the \n",
    "selected model, but was not enforced during the data preparation phase.\n",
    "\n",
    "The removal of attributes was considered to reduce dimensionality and remove potentially redundant information. However,\n",
    "all remaining variables were deemed relevant either as financial indicators, identifiers, or target variables. Therefore, \n",
    "no attributes were removed during data preparation.\n",
    "\n",
    "\"\"\"\n",
    "end_time_nd = now()\n",
    "\n",
    "nd_ass_uuid_executor = \"ec6k81b1-12ab-345z-b9z5-c7d8ab796488\"\n",
    "not_used_executor = [\n",
    "    f':scale_features prov:qualifiedAssociation :{nd_ass_uuid_executor} .',\n",
    "    f':{nd_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{nd_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{nd_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(not_used_executor, prefixes=prefixes)\n",
    "\n",
    "nd_ass_uuid_writer = \"1504f11a-5678-1221-a913-343a3z10l167\"\n",
    "\n",
    "not_used_activity = [\n",
    "    ':not_used rdf:type prov:Activity .',\n",
    "    ':not_used sc:isPartOf :data_preparation_phase .',\n",
    "    ':not_used rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':not_used rdfs:comment \"\"\"{nd_comment}\"\"\" .', \n",
    "    f':not_used prov:startedAtTime \"{start_time_nd}\"^^xsd:dateTime .',\n",
    "    f':not_used prov:endedAtTime \"{end_time_nd}\"^^xsd:dateTime .',\n",
    "    f':not_used prov:qualifiedAssociation :{nd_ass_uuid_writer} .',\n",
    "    f':{nd_ass_uuid_writer} prov:agent :{not_used_code_writer} .',\n",
    "    f':{nd_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{nd_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "engine.insert(not_used_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f577b7-1a2e-4877-92c6-644ca175ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "derived_attributes_code_writer = student_b\n",
    "\n",
    "start_time_da = now()\n",
    "da_comment = \"\"\"\n",
    "The possibility of creating additional derived attributes was considered, such as combining financial ratios or \n",
    "creating summary indicators for liquidity, leverage, or profitability. While these derived features could potentially \n",
    "capture more complex relationships in the data, they were not created at this stage. The original financial ratios already \n",
    "provide sufficient information, and adding derived attributes would increase complexity without big benefits for the current analysis.\n",
    "\"\"\"\n",
    "end_time_da = now()\n",
    "\n",
    "da_ass_uuid_executor = \"uz6k81b1-12gf-345z-b9z5-c7d8ab123488\"\n",
    "derived_attributes_executor = [\n",
    "    f':derived_attributes prov:qualifiedAssociation :{da_ass_uuid_executor} .',\n",
    "    f':{da_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{da_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{da_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(derived_attributes_executor, prefixes=prefixes)\n",
    "\n",
    "da_ass_uuid_writer = \"8464s11a-5678-1221-a913-343y3z10l167\"\n",
    "\n",
    "derived_attributes_activity = [\n",
    "    ':derived_attributes rdf:type prov:Activity .',\n",
    "    ':derived_attributes sc:isPartOf :data_preparation_phase .',\n",
    "    ':derived_attributes rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':derived_attributes rdfs:comment \"\"\"{da_comment}\"\"\" .', \n",
    "    f':derived_attributes prov:startedAtTime \"{start_time_da}\"^^xsd:dateTime .',\n",
    "    f':derived_attributes prov:endedAtTime \"{end_time_da}\"^^xsd:dateTime .',\n",
    "    f':derived_attributes prov:qualifiedAssociation :{da_ass_uuid_writer} .',\n",
    "    f':{da_ass_uuid_writer} prov:agent :{derived_attributes_code_writer} .',\n",
    "    f':{da_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{da_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "engine.insert(derived_attributes_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f2619-8670-4d2f-8e87-82a6e7db76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_sources_code_writer = student_b\n",
    "\n",
    "start_time_ed = now()\n",
    "ed_comment = \"\"\"\n",
    "The use of additional external data sources was considered to improve the prediction of corporate credit risk. \n",
    "Possible external attributes include macroeconomic indicators such as interest rates or economic growth, as well as \n",
    "industry-level information to capture differences between sectors. Market-based data like stock price volatility could \n",
    "also provide useful information. These data sources were not included in this analysis and could be explored in future work.\n",
    "\"\"\"\n",
    "end_time_ed = now()\n",
    "\n",
    "ed_ass_uuid_executor = \"uz6k81b1-12gf-098z-b9c5-c7d8iu123488\"\n",
    "external_data_sourcess_executor = [\n",
    "    f':external_data_sources prov:qualifiedAssociation :{ed_ass_uuid_executor} .',\n",
    "    f':{ed_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{ed_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{ed_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(external_data_sourcess_executor, prefixes=prefixes)\n",
    "\n",
    "ed_ass_uuid_writer = \"8906s12a-5678-1881-a913-343y3v10l167\"\n",
    "\n",
    "external_data_sources_activity = [\n",
    "    ':external_data_sources rdf:type prov:Activity .',\n",
    "    ':external_data_sources sc:isPartOf :data_preparation_phase .',\n",
    "    ':external_data_sources rdfs:comment \\'Data Preparation\\' .', \n",
    "    f':external_data_sources rdfs:comment \"\"\"{ed_comment}\"\"\" .', \n",
    "    f':external_data_sources prov:startedAtTime \"{start_time_ed}\"^^xsd:dateTime .',\n",
    "    f':external_data_sources prov:endedAtTime \"{end_time_ed}\"^^xsd:dateTime .',\n",
    "    f':external_data_sources prov:qualifiedAssociation :{ed_ass_uuid_writer} .',\n",
    "    f':{ed_ass_uuid_writer} prov:agent :{external_data_sources_code_writer} .',\n",
    "    f':{ed_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{ed_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "]\n",
    "engine.insert(external_data_sources_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edd15d-d14a-419c-869e-6a1fa4178826",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_after_code_writer = student_b\n",
    "\n",
    "start_time_va = now()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "if \"current_ratio\" in cleaned_data.columns:\n",
    "    cleaned_data[\"current_ratio\"].hist(ax=axs[0, 0], bins=40)\n",
    "    axs[0, 0].set_title(\"Distribution of Current Ratio\")\n",
    "\n",
    "if \"debt_equity_ratio\" in cleaned_data.columns:\n",
    "    cleaned_data[\"debt_equity_ratio\"].hist(ax=axs[0, 1], bins=40)\n",
    "    axs[0, 1].set_title(\"Distribution of Debt to Equity Ratio\")\n",
    "\n",
    "if \"roa___return_on_assets\" in cleaned_data.columns:\n",
    "    cleaned_data[\"roa___return_on_assets\"].hist(ax=axs[1, 0], bins=40)\n",
    "    axs[1, 0].set_title(\"Distribution of ROA\")\n",
    "\n",
    "if \"rating\" in cleaned_data.columns:\n",
    "    cleaned_data[\"rating\"].value_counts().sort_index().plot(kind=\"bar\", ax=axs[1, 1])\n",
    "    axs[1, 1].set_title(\"Rating Class Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "end_time_va = now()\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "visual_after_comment = \"\"\"\n",
    "Produce histograms for selected financial ratios and a bar chart for rating classes.\n",
    "The plots show that outliers have been removed.\n",
    "\"\"\"\n",
    "\n",
    "va_ass_uuid_executor = \"aaeb7ab9-c9f1-4b99-a248-123a9bfc7ae7\"\n",
    "visual_after_executor_triples = [\n",
    "    f':visual_exploration prov:qualifiedAssociation :{va_ass_uuid_executor} .',\n",
    "    f':{va_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "    f':{va_ass_uuid_executor} rdf:type prov:Association .',\n",
    "    f':{va_ass_uuid_executor} prov:hadRole :{code_executor_role} .',\n",
    "]\n",
    "engine.insert(visual_after_executor_triples, prefixes=prefixes)\n",
    "\n",
    "va_ass_uuid_writer = \"64c6a0ef-5ce3-4ad0-9efb-123z5a03c400\"\n",
    "visual_after_activity = [\n",
    "    ':visual_exploration_after_cleaning rdf:type prov:Activity .',\n",
    "    ':visual_exploration_after_cleaning sc:isPartOf :data_understanding_phase .',\n",
    "    ':visual_exploration_after_cleaning rdfs:comment \\'Data Understanding\\' .',\n",
    "    f':visual_exploration_after_cleaning rdfs:comment \"\"\"{visual_after_comment}\"\"\" .',\n",
    "    f':visual_exploration_after_cleaning prov:startedAtTime \"{start_time_va}\"^^xsd:dateTime .',\n",
    "    f':visual_exploration_after_cleaning prov:endedAtTime \"{end_time_va}\"^^xsd:dateTime .',\n",
    "    ':visual_exploration_after_cleaning prov:used :data .',\n",
    "    f':visual_exploration_after_cleaning prov:qualifiedAssociation :{va_ass_uuid_writer} .',\n",
    "    f':{va_ass_uuid_writer} prov:agent :{visual_after_code_writer} .',\n",
    "    f':{va_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{va_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    ':visual_exploration_report rdf:type prov:Entity .',\n",
    "    ':visual_exploration_report rdfs:label \"Visual Exploration Summary\" .',\n",
    "    ':visual_exploration_report rdfs:comment \"\"\"Histograms for key ratios and rating distribution.\"\"\" .',\n",
    "    ':visual_exploration_report prov:wasGeneratedBy :visual_exploration .',\n",
    "]\n",
    "engine.insert(visual_after_activity, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0036428-fcdf-4ee8-ad52-424f95024cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your final transformed dataset should also be documented appropriately using Croissant, SI, etc.\n",
    "prepared_data = cleaned_data\n",
    "prepared_data_triples = [\n",
    "    ':prepared_data rdf:type prov:Entity .',\n",
    "    ':prepared_data prov:wasDerivedFrom :cleaned_data .',\n",
    "    ':prepared_data prov:wasGeneratedBy :data_preparation_phase .',\n",
    "    ':prepared_data rdf:type sc:Dataset .',\n",
    "    ':prepared_data sc:name \"Prepared Corporate Credit Dataset\" .',\n",
    "    ':prepared_data sc:description \"Final prepared dataset containing cleaned financial ratios and encoded credit rating targets for corporate credit risk modeling.\" .',\n",
    "]\n",
    "engine.insert(prepared_data_triples, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c19ebb",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb93dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Modeling Phase\n",
    "\n",
    "modeling_phase_executor = [\n",
    "f':modeling_phase rdf:type prov:Activity .',\n",
    "f':modeling rdfs:label \"Modeling Phase\" .', \n",
    "]\n",
    "engine.insert(modeling_phase_executor, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a090e1-28ab-455d-b325-b169a5da4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepared_data.copy()\n",
    "\n",
    "TARGET = \"rating\"\n",
    "HELPER_TARGET = \"binary_rating\"\n",
    "\n",
    "assert TARGET in df.columns, \"Target column 'rating' missing in prepared_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data_code_writer = student_a\n",
    "\n",
    "#############################################\n",
    "# Documentation 4a\n",
    "#############################################\n",
    "\n",
    "dma_ass_uuid_writer = \"773b57cd-2978-4347-8729-8791b73f7395\"\n",
    "dma_comment = \"\"\"\n",
    "We identify suitable algorithms for predicting the multi-class corporate credit rating from financial ratios.\n",
    "\n",
    "Candidates:\n",
    "1) Multinomial Logistic Regression (baseline)\n",
    "   - Fast baseline for multi-class classification.\n",
    "   - Works best if relations are close to linear.\n",
    "   - Needs feature scaling.\n",
    "\n",
    "2) Random Forest Classifier\n",
    "   - Strong baseline for tabular numeric ratios.\n",
    "   - Captures non-linear relations and feature interactions.\n",
    "   - Robust with class_weight for imbalance.\n",
    "   - Supports model interpretation via feature importance.\n",
    "\n",
    "3) Histogram-based Gradient Boosting Classifier\n",
    "   - Often strong on tabular data.\n",
    "   - Captures non-linear effects.\n",
    "   - Can outperform Random Forest with limited tuning.\n",
    "\n",
    "Selection criterion:\n",
    "We select the model with the best validation Macro F1 score.\n",
    "Also report validation accuracy as a secondary metric.\n",
    "\"\"\"\n",
    "\n",
    "identify_data_mining_algorithm_activity = [\n",
    "    f':define_algorithm rdf:type prov:Activity .',\n",
    "    f':define_algorithm sc:isPartOf :modeling_phase .',\n",
    "    f':define_algorithm rdfs:comment \"\"\"{dma_comment}\"\"\" .',\n",
    "\n",
    "    f':define_algorithm prov:qualifiedAssociation :{dma_ass_uuid_writer} .',\n",
    "    f':{dma_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{dma_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{dma_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # algorithm definition\n",
    "    f':logreg_algorithm rdf:type mls:Algorithm .',\n",
    "    f':logreg_algorithm rdfs:label \"Multinomial Logistic Regression\" .',\n",
    "    \n",
    "    f':random_forest_algorithm rdf:type mls:Algorithm .',\n",
    "    f':random_forest_algorithm rdfs:label \"Random Forest Algorithm\" .',\n",
    "\n",
    "    f':hist_gradient_boosting_algorithm rdf:type mls:Algorithm .',\n",
    "    f':hist_gradient_boosting_algorithm rdfs:label \"Histogram-based Gradient Boosting Classifier\" .',\n",
    "\n",
    "    \n",
    "    f':selected_model_implementation rdf:type mls:Model .',\n",
    "    f':selected_model_implementation rdfs:label \"Selected Model Implementation\" .',\n",
    "    f':selected_model_implementation prov:wasGeneratedBy :define_algorithm .',\n",
    "\n",
    "    f':macro_f1_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':macro_f1_measure rdfs:label \"Macro F1\" .',\n",
    "    f':accuracy_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':accuracy_measure rdfs:label \"Accuracy\" .',\n",
    "    \n",
    "]\n",
    "engine.insert(identify_data_mining_algorithm_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef613f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation 4b\n",
    "#############################################\n",
    "\n",
    "hp_ass_uuid_writer = \"e9848ebc-e283-49b2-9205-fb340f7bd05c\"\n",
    "hp_comment = \"\"\"\n",
    "We identify relevant hyperparameters for Random Forest.\n",
    "\n",
    "Relevant hyperparameters:\n",
    "- n_estimators: number of trees\n",
    "- max_depth: maximum tree depth, controls model complexity and overfitting\n",
    "- min_samples_split and min_samples_leaf: regularize splits\n",
    "- max_features: number of features considered per split\n",
    "- class_weight: handle class imbalance\n",
    "\n",
    "Chosen tuning hyperparameter:\n",
    "- max_depth\n",
    "Reason:\n",
    "- Direct control over overfitting and complexity.\n",
    "- Simple grid search, easy to document and reproduce.\n",
    "\n",
    "Fixed parameters for tuning:\n",
    "- n_estimators = 300\n",
    "- class_weight = balanced\n",
    "- random_state = 42\n",
    "\"\"\"\n",
    "\n",
    "identify_hp_activity = [\n",
    "    f':hp_n_estimators rdf:type mls:HyperParameter .',\n",
    "    f':hp_n_estimators rdfs:label \"n_estimators\" .',\n",
    "    f':hp_n_estimators rdfs:comment \"Number of trees in the Random Forest.\" .',\n",
    "\n",
    "    f':hp_max_depth rdf:type mls:HyperParameter .',\n",
    "    f':hp_max_depth rdfs:label \"max_depth\" .',\n",
    "    f':hp_max_depth rdfs:comment \"Maximum depth of each decision tree, controls complexity and overfitting.\" .',\n",
    "\n",
    "    f':hp_min_samples_split rdf:type mls:HyperParameter .',\n",
    "    f':hp_min_samples_split rdfs:label \"min_samples_split\" .',\n",
    "    f':hp_min_samples_split rdfs:comment \"Minimum number of samples required to split an internal node.\" .',\n",
    "\n",
    "    f':hp_min_samples_leaf rdf:type mls:HyperParameter .',\n",
    "    f':hp_min_samples_leaf rdfs:label \"min_samples_leaf\" .',\n",
    "    f':hp_min_samples_leaf rdfs:comment \"Minimum number of samples required to be at a leaf node.\" .',\n",
    "\n",
    "    f':hp_max_features rdf:type mls:HyperParameter .',\n",
    "    f':hp_max_features rdfs:label \"max_features\" .',\n",
    "    f':hp_max_features rdfs:comment \"Number of features considered when looking for the best split.\" .',\n",
    "\n",
    "    f':hp_class_weight rdf:type mls:HyperParameter .',\n",
    "    f':hp_class_weight rdfs:label \"class_weight\" .',\n",
    "    f':hp_class_weight rdfs:comment \"Weights associated with classes, used to handle class imbalance.\" .',\n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(identify_hp_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995966b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_data(df: pd.DataFrame):\n",
    "    TARGET = \"rating\"\n",
    "    assert TARGET in df.columns, \"Target column 'rating' missing\"\n",
    "\n",
    "    d = df.copy()\n",
    "    d = d.replace([np.inf, -np.inf], np.nan)\n",
    "    d = d.dropna(subset=[TARGET]).copy()\n",
    "    d[TARGET] = d[TARGET].astype(int)\n",
    "\n",
    "    drop_cols = [TARGET]\n",
    "    if \"binary_rating\" in d.columns:\n",
    "        drop_cols.append(\"binary_rating\")\n",
    "    for c in [\"ticker\", \"corporation\", \"sector\", \"rating_agency\", \"rating_date\", \"company\"]:\n",
    "        if c in d.columns:\n",
    "            drop_cols.append(c)\n",
    "\n",
    "    feature_df = d.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    feature_df = feature_df.select_dtypes(include=[\"number\"]).copy()\n",
    "\n",
    "    modelling_df = pd.concat([feature_df, d[[TARGET]]], axis=1)\n",
    "    modelling_df = modelling_df.dropna(axis=0).copy()\n",
    "\n",
    "    X = modelling_df.drop(columns=[TARGET])\n",
    "    y = modelling_df[TARGET].copy()\n",
    "\n",
    "    min_per_class = 11\n",
    "    class_counts = y.value_counts()\n",
    "    keep_classes = class_counts[class_counts >= min_per_class].index\n",
    "    X = X[y.isin(keep_classes)].copy()\n",
    "    y = y[y.isin(keep_classes)].copy()\n",
    "\n",
    "    assert y.nunique() > 1, \"Need at least 2 classes after filtering\"\n",
    "\n",
    "    X_rest, X_test, y_rest, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.15,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    val_frac_of_rest = 0.15 / 0.85\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_rest, y_rest,\n",
    "        test_size=val_frac_of_rest,\n",
    "        random_state=42,\n",
    "        stratify=y_rest\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(prepared_data)\n",
    "\n",
    "n_train = len(X_train)\n",
    "n_val = len(X_val)\n",
    "n_test = len(X_test)\n",
    "\n",
    "print(n_train, n_val, n_test)\n",
    "print(\"Classes:\", y_train.nunique(), y_val.nunique(), y_test.nunique())\n",
    "print(\"Min class count in train:\", y_train.value_counts().min())\n",
    "print(\"Min class count in val:\", y_val.value_counts().min())\n",
    "print(\"Min class count in test:\", y_test.value_counts().min())\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4c\n",
    "#############################################\n",
    "\n",
    "### Define Train/Validation/Test splits\n",
    "split_ass_uuid_writer = \"6ad505bc-e815-4e92-94cd-8a8995a9bd6c\"\n",
    "\n",
    "min_per_class_used = 11\n",
    "val_frac_of_rest = 0.15 / 0.85\n",
    "\n",
    "split_comment = f\"\"\"\n",
    "Train/Validation/Test split performed on prepared_data.\n",
    "\n",
    "Pre-split cleaning and preparation:\n",
    "- Replaced +/-inf with NaN globally.\n",
    "- Dropped rows with missing target 'rating'.\n",
    "- Kept numeric feature columns only.\n",
    "- Dropped rows with missing numeric feature values.\n",
    "- Filtered classes with fewer than {min_per_class_used} samples to ensure stable stratified splitting.\n",
    "\n",
    "Split strategy:\n",
    "- Step 1: Split off test set with test_size=0.15, stratified by 'rating', random_state=42.\n",
    "- Step 2: Split the remaining 85% into training and validation.\n",
    "  Validation fraction of remaining data: {val_frac_of_rest:.6f}, stratified by 'rating', random_state=42.\n",
    "- Final ratio: 70% training, 15% validation, 15% test.\n",
    "\n",
    "Resulting sizes:\n",
    "- Training set: {n_train} samples\n",
    "- Validation set: {n_val} samples\n",
    "- Test set: {n_test} samples\n",
    "\n",
    "Class distribution sanity check:\n",
    "- Classes in train/val/test: {y_train.nunique()} / {y_val.nunique()} / {y_test.nunique()}\n",
    "- Minimum class count in train: {int(y_train.value_counts().min())}\n",
    "- Minimum class count in validation: {int(y_val.value_counts().min())}\n",
    "- Minimum class count in test: {int(y_test.value_counts().min())}\n",
    "\"\"\"\n",
    "\n",
    "## Use your prepared dataset\n",
    "input_dataset = \":prepared_data\" \n",
    "\n",
    "define_split_activity = [\n",
    "    f':define_data_split rdf:type prov:Activity .',\n",
    "    f':define_data_split sc:isPartOf :modeling_phase .',\n",
    "    f':define_data_split rdfs:comment \"Train/Validation/Test Split Definition\" .',\n",
    "    f':define_data_split rdfs:comment \"\"\"{split_comment}\"\"\" .',\n",
    "    f':define_data_split prov:qualifiedAssociation :{split_ass_uuid_writer} .',\n",
    "    f':{split_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{split_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{split_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "    f':define_data_split prov:used {input_dataset} .',\n",
    "    \n",
    "    # Training Set\n",
    "    f':training_set rdf:type sc:Dataset .',\n",
    "    f':training_set rdfs:label \"Training Set\" .',\n",
    "    f':training_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':training_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':training_set rdfs:comment \"Contains {n_train} samples\" .', \n",
    "\n",
    "    # Validation Set\n",
    "    f':validation_set rdf:type sc:Dataset .',\n",
    "    f':validation_set rdfs:label \"Validation Set\" .',\n",
    "    f':validation_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':validation_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':validation_set rdfs:comment \"Contains {n_val} samples\" .', \n",
    "\n",
    "    # Test Set\n",
    "    f':test_set rdf:type sc:Dataset .',\n",
    "    f':test_set rdfs:label \"Test Set\" .',\n",
    "    f':test_set prov:wasGeneratedBy :define_data_split .',\n",
    "    f':test_set prov:wasDerivedFrom {input_dataset} .',\n",
    "    f':test_set rdfs:comment \"Contains {n_test} samples\" .', \n",
    "\n",
    "    \n",
    "]\n",
    "engine.insert(define_split_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b5ed6-54d6-4c81-9adb-e295fbd5c364",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-978b274ef875c238",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_finetune_model(X_train, y_train, X_val, y_val):\n",
    "    depth_grid = [None, 5, 10, 15, 20]\n",
    "    \n",
    "    fixed_params = {\n",
    "        \"n_estimators\": 300,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    runs = []\n",
    "    best_depth = None\n",
    "    best_val_f1 = -1.0\n",
    "    best_model = None\n",
    "\n",
    "    for depth in depth_grid:\n",
    "        model = RandomForestClassifier(max_depth=depth, **fixed_params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        pred_train = model.predict(X_train)\n",
    "        pred_val = model.predict(X_val)\n",
    "\n",
    "        train_f1 = f1_score(y_train, pred_train, average=\"macro\")\n",
    "        val_f1 = f1_score(y_val, pred_val, average=\"macro\")\n",
    "\n",
    "        train_acc = accuracy_score(y_train, pred_train)\n",
    "        val_acc = accuracy_score(y_val, pred_val)\n",
    "\n",
    "        runs.append({\n",
    "            \"max_depth\": \"None\" if depth is None else int(depth),\n",
    "            \"train_macro_f1\": float(train_f1),\n",
    "            \"val_macro_f1\": float(val_f1),\n",
    "            \"train_accuracy\": float(train_acc),\n",
    "            \"val_accuracy\": float(val_acc),\n",
    "        })\n",
    "\n",
    "        if float(val_f1) > best_val_f1:\n",
    "            best_val_f1 = float(val_f1)\n",
    "            best_depth = depth\n",
    "            best_model = model\n",
    "\n",
    "    results_df = pd.DataFrame(runs).sort_values(\"val_macro_f1\", ascending=False).reset_index(drop=True)\n",
    "    return best_model, best_depth, best_val_f1, results_df\n",
    "    \n",
    "    # Try to automate as much documentation work as possible.\n",
    "    # Define your training runs with their respective hyperparameter settings, etc.\n",
    "    # Document each time a training run, model, its hp_settings, evaluations, ...  \n",
    "    # Create performance figures/graphs\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "best_model, best_depth, best_val_f1, tuning_results = train_and_finetune_model(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "end_time_tafm = now()\n",
    "\n",
    "display(tuning_results)\n",
    "print(\"Selected best_depth:\", \"None\" if best_depth is None else best_depth)\n",
    "print(\"Best validation macro F1:\", best_val_f1)\n",
    "\n",
    "plot_df = tuning_results.copy()\n",
    "\n",
    "# map \"None\" to a large depth for ordering, then sort\n",
    "plot_df[\"_depth_num\"] = plot_df[\"max_depth\"].apply(lambda x: 999 if str(x) == \"None\" else int(x))\n",
    "plot_df = plot_df.sort_values(\"_depth_num\", ascending=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_df[\"max_depth\"].astype(str), plot_df[\"val_macro_f1\"], marker=\"o\")\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Validation Macro F1\")\n",
    "plt.title(\"Random Forest Tuning: max_depth vs Validation Macro F1\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation 4d & e & f\n",
    "#############################################\n",
    "\n",
    "tafm_ass_uuid_writer = \"a75da300-1e5a-4341-9de0-5bfaf753825a\"\n",
    "\n",
    "selected_depth_label = \"None\" if best_depth is None else str(best_depth)\n",
    "\n",
    "tafm_comment = f\"\"\"\n",
    "We trained and fine-tuned a Random Forest Classifier using the Train and Validation split created in 4c.\n",
    "We performed a small grid search over max_depth to control model complexity.\n",
    "\n",
    "Fixed hyperparameters:\n",
    "- n_estimators = 300\n",
    "- class_weight = balanced\n",
    "- random_state = 42\n",
    "- n_jobs = -1\n",
    "\n",
    "Tuned hyperparameter:\n",
    "- max_depth  {list(tuning_results['max_depth'].astype(str).unique())}\n",
    "\n",
    "Evaluation:\n",
    "- Macro F1 and Accuracy on training and validation sets.\n",
    "\n",
    "Selection:\n",
    "- The final model is selected by highest validation Macro F1.\n",
    "- Selected max_depth = {selected_depth_label}\n",
    "- Best validation Macro F1 = {best_val_f1:.6f}\n",
    "\"\"\"\n",
    "\n",
    "# EXAMPLE output from your training\n",
    "training_run1 = \"run_1\" \n",
    "model_run1 = \"model_run1\"\n",
    "hp1_setting_run1 = \"hp_setting_run1\"\n",
    "eval_train_run1 = \"metric_train_run1\"\n",
    "eval_validation_run1 = \"metric_validation_run1\"\n",
    "\n",
    "\n",
    "train_model_activity = [\n",
    "    # Activity \n",
    "    f':train_and_finetune_model rdf:type prov:Activity .',\n",
    "    f':train_and_finetune_model sc:isPartOf :modeling_phase .',\n",
    "    f':train_and_finetune_model rdfs:comment \"\"\"{tafm_comment}\"\"\" .',\n",
    "    f':train_and_finetune_model prov:startedAtTime \"{start_time_tafm}\"^^xsd:dateTime .',\n",
    "    f':train_and_finetune_model prov:endedAtTime \"{end_time_tafm}\"^^xsd:dateTime .',\n",
    "\n",
    "\n",
    "    f':train_and_finetune_model prov:qualifiedAssociation :{tafm_ass_uuid_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{tafm_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{tafm_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    f':macro_f1_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':macro_f1_measure rdfs:label \"Macro F1\" .',\n",
    "\n",
    "    f':accuracy_measure rdf:type mls:EvaluationMeasure .',\n",
    "    f':accuracy_measure rdfs:label \"Accuracy\" .',\n",
    "\n",
    "    # Inputs\n",
    "    f':train_and_finetune_model prov:used :training_set .',\n",
    "    f':train_and_finetune_model prov:used :validation_set .',\n",
    "]\n",
    "engine.insert(train_model_activity, prefixes=prefixes)\n",
    "\n",
    "for i, row in tuning_results.iterrows():\n",
    "    run_idx = i + 1\n",
    "    depth_label = str(row[\"max_depth\"])\n",
    "\n",
    "    training_run = f\"rf_run_{run_idx}\"\n",
    "    model_entity = f\"rf_model_{run_idx}\"\n",
    "\n",
    "    # Hyperparameter setting entity\n",
    "    hp_setting = f\"rf_hp_setting_{run_idx}_max_depth\"\n",
    "\n",
    "    # Evaluations: Macro F1 and Accuracy on Train and Val\n",
    "    eval_train_f1 = f\"rf_eval_{run_idx}_train_macro_f1\"\n",
    "    eval_val_f1 = f\"rf_eval_{run_idx}_val_macro_f1\"\n",
    "    eval_train_acc = f\"rf_eval_{run_idx}_train_accuracy\"\n",
    "    eval_val_acc = f\"rf_eval_{run_idx}_val_accuracy\"\n",
    "\n",
    "    # Mark selected model/run\n",
    "    is_selected = (depth_label == selected_depth_label)\n",
    "    selected_tag = \"SELECTED_MODEL\" if is_selected else \"NOT_SELECTED\"\n",
    "\n",
    "    triples = [\n",
    "        ########################################\n",
    "        # Parameter settings (HyperParameterSetting)\n",
    "        f':{hp_setting} rdf:type mls:HyperParameterSetting .',\n",
    "        f':{hp_setting} mls:specifiedBy :hp_max_depth .',\n",
    "        f':{hp_setting} mls:hasValue \"{depth_label}\" .',\n",
    "        f':{hp_setting} prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "\n",
    "        ########################################\n",
    "        # Describe the Run (mls:Run)\n",
    "        f':{training_run} rdf:type mls:Run .',\n",
    "        f':{training_run} sc:isPartOf :train_and_finetune_model .',\n",
    "        f':{training_run} mls:realizes :random_forest_algorithm .',\n",
    "        f':{training_run} rdfs:label \"RF Training Run {run_idx} (max_depth={depth_label})\" .',\n",
    "\n",
    "        # Inputs for this run\n",
    "        f':{training_run} mls:hasInput :training_set .',\n",
    "        f':{training_run} mls:hasInput :validation_set .',\n",
    "        f':{training_run} mls:hasInput :{hp_setting} .',\n",
    "\n",
    "        # Outputs for this run\n",
    "        f':{training_run} mls:hasOutput :{model_entity} .',\n",
    "        f':{training_run} mls:hasOutput :{eval_train_f1} .',\n",
    "        f':{training_run} mls:hasOutput :{eval_val_f1} .',\n",
    "        f':{training_run} mls:hasOutput :{eval_train_acc} .',\n",
    "        f':{training_run} mls:hasOutput :{eval_val_acc} .',\n",
    "\n",
    "        ########################################\n",
    "        # Describe the Model (mls:Model)\n",
    "        f':{model_entity} rdf:type mls:Model .',\n",
    "        f':{model_entity} rdfs:label \"Random Forest Model {run_idx}\" .',\n",
    "        f':{model_entity} prov:wasGeneratedBy :{training_run} .',\n",
    "        f':{model_entity} mlso:trainedOn :training_set .',\n",
    "        f':{model_entity} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "        f':{model_entity} rdfs:comment \"{selected_tag}\" .',\n",
    "\n",
    "        ########################################\n",
    "        # Evaluations (mls:ModelEvaluation)\n",
    "\n",
    "        # Train Macro F1\n",
    "        f':{eval_train_f1} rdf:type mls:ModelEvaluation .',\n",
    "        f':{eval_train_f1} prov:wasGeneratedBy :{training_run} .',\n",
    "        f':{eval_train_f1} mls:specifiedBy :macro_f1_measure .',\n",
    "        f':{eval_train_f1} mls:hasValue \"{row[\"train_macro_f1\"]:.6f}\"^^xsd:double .',\n",
    "        f':{eval_train_f1} prov:used :training_set .',\n",
    "\n",
    "        # Val Macro F1\n",
    "        f':{eval_val_f1} rdf:type mls:ModelEvaluation .',\n",
    "        f':{eval_val_f1} prov:wasGeneratedBy :{training_run} .',\n",
    "        f':{eval_val_f1} mls:specifiedBy :macro_f1_measure .',\n",
    "        f':{eval_val_f1} mls:hasValue \"{row[\"val_macro_f1\"]:.6f}\"^^xsd:double .',\n",
    "        f':{eval_val_f1} prov:used :validation_set .',\n",
    "\n",
    "        # Train Accuracy\n",
    "        f':{eval_train_acc} rdf:type mls:ModelEvaluation .',\n",
    "        f':{eval_train_acc} prov:wasGeneratedBy :{training_run} .',\n",
    "        f':{eval_train_acc} mls:specifiedBy :accuracy_measure .',\n",
    "        f':{eval_train_acc} mls:hasValue \"{row[\"train_accuracy\"]:.6f}\"^^xsd:double .',\n",
    "        f':{eval_train_acc} prov:used :training_set .',\n",
    "\n",
    "        # Val Accuracy\n",
    "        f':{eval_val_acc} rdf:type mls:ModelEvaluation .',\n",
    "        f':{eval_val_acc} prov:wasGeneratedBy :{training_run} .',\n",
    "        f':{eval_val_acc} mls:specifiedBy :accuracy_measure .',\n",
    "        f':{eval_val_acc} mls:hasValue \"{row[\"val_accuracy\"]:.6f}\"^^xsd:double .',\n",
    "        f':{eval_val_acc} prov:used :validation_set .',\n",
    "    ]\n",
    "\n",
    "    engine.insert(triples, prefixes=prefixes)\n",
    "\n",
    "# Visualization entity (4e)\n",
    "viz_comment = \"Random Forest tuning curve showing Validation Macro F1 for different max_depth values.\"\n",
    "engine.insert([\n",
    "    f':rf_tuning_figure rdf:type sc:Visualization .',\n",
    "    f':rf_tuning_figure rdfs:label \"RF Tuning Curve\" .',\n",
    "    f':rf_tuning_figure rdfs:comment \"{viz_comment}\" .',\n",
    "    f':rf_tuning_figure prov:wasGeneratedBy :train_and_finetune_model .',\n",
    "], prefixes=prefixes)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def retrain_model_full_data(X_train, y_train, X_val, y_val, best_depth):\n",
    "    X_full = pd.concat([X_train, X_val], axis=0)\n",
    "    y_full = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "    fixed_params = {\n",
    "        \"n_estimators\": 300,\n",
    "        \"class_weight\": \"balanced\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    final_model = RandomForestClassifier(max_depth=best_depth, **fixed_params)\n",
    "    final_model.fit(X_full, y_full)\n",
    "    return final_model, X_full, y_full\n",
    "\n",
    "\n",
    "start_time_tafm = now()\n",
    "final_model_obj, X_full, y_full = retrain_model_full_data(\n",
    "    X_train, y_train, X_val, y_val, best_depth\n",
    ")\n",
    "end_time_tafm = now() \n",
    "\n",
    "print(\"Final model trained on train+validation\")\n",
    "print(\"max_depth:\", \"None\" if best_depth is None else best_depth)\n",
    "print(\"Full training size:\", len(X_full))\n",
    "print(\"Classes in full training:\", y_full.nunique())\n",
    "\n",
    "#############################################\n",
    "# Documentation 4g\n",
    "#############################################\n",
    "\n",
    "retrain_ass_uuid_writer = \"89880614-d9d2-4207-a197-441a4e8a156e\" # Generate once\n",
    "\n",
    "final_training_activity = \":retrain_final_model\"\n",
    "final_model_entity = \":final_model_entity\"\n",
    "full_train_dataset = \":training_validation_set\"\n",
    "\n",
    "selected_depth_label = \"None\" if best_depth is None else str(best_depth)\n",
    "\n",
    "start_time_retrain = start_time_tafm\n",
    "end_time_retrain = end_time_tafm\n",
    "\n",
    "# Document the retraining activity.\n",
    "# Hint: This activity is still part of the :modeling_phase\n",
    "\n",
    "retrain_comment = f\"\"\"\n",
    "Final retraining step (4g) after hyperparameter tuning and model selection (4d-4f).\n",
    "\n",
    "Goal:\n",
    "- Train the final model on all available labeled data excluding the test set.\n",
    "\n",
    "Training data:\n",
    "- Combined training_set and validation_set (from 4c) into one dataset.\n",
    "\n",
    "Algorithm:\n",
    "- Random Forest Classifier\n",
    "\n",
    "Hyperparameters:\n",
    "- max_depth = {selected_depth_label} (selected in 4f)\n",
    "- n_estimators = 300\n",
    "- class_weight = balanced\n",
    "- random_state = 42\n",
    "- n_jobs = -1\n",
    "\n",
    "Full training size (train+validation):\n",
    "- {len(X_full)} samples\n",
    "- {y_full.nunique()} classes\n",
    "\"\"\"\n",
    "\n",
    "retrain_documentation = [\n",
    "    f'{final_training_activity} rdf:type prov:Activity .',\n",
    "    f'{final_training_activity} sc:isPartOf :modeling_phase .',\n",
    "    f'{final_training_activity} rdfs:comment \"\"\"{retrain_comment}\"\"\" .',\n",
    "    f'{final_training_activity} prov:startedAtTime \"{start_time_retrain}\"^^xsd:dateTime .',\n",
    "    f'{final_training_activity} prov:endedAtTime \"{end_time_retrain}\"^^xsd:dateTime .',\n",
    "\n",
    "    f'{final_training_activity} prov:qualifiedAssociation :{retrain_ass_uuid_writer} .',\n",
    "    f':{retrain_ass_uuid_writer} prov:agent :{model_data_code_writer} .',\n",
    "    f':{retrain_ass_uuid_writer} rdf:type prov:Association .',\n",
    "    f':{retrain_ass_uuid_writer} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    f'{final_training_activity} prov:used :training_set .',\n",
    "    f'{final_training_activity} prov:used :validation_set .',\n",
    "\n",
    "    f'{full_train_dataset} rdf:type sc:Dataset .',\n",
    "    f'{full_train_dataset} rdfs:label \"Training+Validation Set\" .',\n",
    "    f'{full_train_dataset} prov:wasGeneratedBy {final_training_activity} .',\n",
    "    f'{full_train_dataset} prov:wasDerivedFrom :training_set .',\n",
    "    f'{full_train_dataset} prov:wasDerivedFrom :validation_set .',\n",
    "    f'{full_train_dataset} rdfs:comment \"Contains {len(X_full)} samples (combined from training and validation).\" .',\n",
    "\n",
    "    f':final_hp_setting_max_depth rdf:type mls:HyperParameterSetting .',\n",
    "    f':final_hp_setting_max_depth mls:specifiedBy :hp_max_depth .',\n",
    "    f':final_hp_setting_max_depth mls:hasValue \"{selected_depth_label}\" .',\n",
    "    f':final_hp_setting_max_depth prov:wasGeneratedBy {final_training_activity} .',\n",
    "\n",
    "    f':final_training_run rdf:type mls:Run .',\n",
    "    f':final_training_run sc:isPartOf {final_training_activity} .',\n",
    "    f':final_training_run mls:realizes :random_forest_algorithm .',\n",
    "    f':final_training_run rdfs:label \"Final RF training run on train+validation (max_depth={selected_depth_label})\" .',\n",
    "    f':final_training_run mls:hasInput {full_train_dataset} .',\n",
    "    f':final_training_run mls:hasInput :final_hp_setting_max_depth .',\n",
    "    f':final_training_run mls:hasOutput {final_model_entity} .',\n",
    "\n",
    "    f'{final_model_entity} rdf:type mls:Model .',\n",
    "    f'{final_model_entity} rdfs:label \"Final Random Forest Model\" .',\n",
    "    f'{final_model_entity} prov:wasGeneratedBy :final_training_run .',\n",
    "    f'{final_model_entity} mlso:trainedOn {full_train_dataset} .',\n",
    "    f'{final_model_entity} mlso:hasAlgorithmType :random_forest_algorithm .',\n",
    "    f'{final_model_entity} rdfs:comment \"Final model trained with max_depth={selected_depth_label}, n_estimators=300, class_weight=balanced, random_state=42.\" .',\n",
    "\n",
    "    \n",
    "    \n",
    "]\n",
    "engine.insert(retrain_documentation, prefixes=prefixes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06583f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a88bf71f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46137067",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Evaluation Phase\n",
    "\n",
    "evaluation_phase_executor = [\n",
    "f':evaluation_phase rdf:type prov:Activity .',\n",
    "f':evaluation_phase rdfs:label \"Evaluation Phase\" .', \n",
    "]\n",
    "engine.insert(evaluation_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d80e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_code_writer = student_b\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, mean_absolute_error, cohen_kappa_score, precision_score, recall_score\n",
    "def evaluate_on_test_data(final_model, test_set):\n",
    "    # Predict and evaluation on test data\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    pred_test = final_model.predict(X_test)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, pred_test)\n",
    "    plt.show()\n",
    "\n",
    "    return  {\n",
    "        \"f1\": f1_score(y_test, pred_test, average=\"macro\"),\n",
    "        \"accuracy\": accuracy_score(y_test, pred_test),\n",
    "        \"precision\": precision_score(y_test, pred_test, average=\"macro\"),\n",
    "        \"recall\": recall_score(y_test, pred_test, average=\"macro\"),\n",
    "        \"mae\": mean_absolute_error(y_test, pred_test),\n",
    "        \"qwk\": cohen_kappa_score(y_test, pred_test, weights=\"quadratic\"),\n",
    "        \"plusMinus1Accuracy\": np.mean(np.abs(y_test - pred_test) <= 1)\n",
    "    }\n",
    "def evaluate_bias_by_sic(test_set,model):\n",
    "    X_test = test_set[0]\n",
    "    y_test = test_set[1]\n",
    "    sic_col_idx = 1\n",
    "    rows = []\n",
    "\n",
    "    sic_codes = X_test.iloc[:, sic_col_idx].astype(int)\n",
    "    sic_groups = sic_codes // 1000  # 1-digit SIC\n",
    "\n",
    "    for sic_group in np.unique(sic_groups):\n",
    "        mask = sic_groups == sic_group\n",
    "\n",
    "        #if mask.sum() < min_samples:\n",
    "         #   continue\n",
    "\n",
    "        X_s = X_test[mask]\n",
    "        y_true = y_test[mask]\n",
    "        y_pred = model.predict(X_s)\n",
    "\n",
    "        rows.append({\n",
    "            \"SIC_Group\": sic_group,\n",
    "            \"Samples\": mask.sum(),\n",
    "            \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "            \"PlusMinus1\": np.mean(np.abs(y_true - y_pred) <= 1),\n",
    "            \"DirectionalBias\": np.mean(y_pred - y_true)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values(\"MAE\")\n",
    "    \n",
    "test_set = [X_test, y_test]\n",
    "\n",
    "start_time_eval = now()\n",
    "metrics = evaluate_on_test_data(final_model_obj, test_set)\n",
    "bias_eval = evaluate_bias_by_sic(test_set, final_model_obj)\n",
    "end_time_eval = now() \n",
    "\n",
    "for name, value in metrics.items():\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "print(bias_eval)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "eval_ass_uuid = \"7f1631e9-feed-87za-92ed-c131b23cbe79\" # Generate once\n",
    "final_model = \"final_model_obj\" \n",
    "test_set = \":test_set\" \n",
    "eval_comment = \"\"\" \n",
    "In this part we evaluate our model. We look at the performance,compare it to state of the art and random classifiers and at at bias. \n",
    "\"\"\"\n",
    "performance_comment = \"\"\"\n",
    "To evaluate the performance of our model we have to look at different metrics. The accuracy of our model is only 0.4553 \n",
    "this means only 45% of the classifications are correct. But this can be explained. We have 18 different classes and these classes are ordinal. \n",
    "A wrong classification is not so bad if it is not far away from the correct class. So we have to look at the distances. \n",
    "One way to do this is to look how many classifications have a distance of 1 or less, so the are in the right or in one of the neighboring classes.\n",
    "In this case our accuracy is 0.7191 which means nearly 72 percent of our classification do have a maximum distance of 1 from the right class. \n",
    "Other measures to look at are the Mean Absolute Error (MAE) and Quadratic Weighted Kappa (QWK). \n",
    "The Mean Absolute Error is the average of the distances. For our model the mea is 1.15, this means on average the classification is 1.15 classes away from the true class.\n",
    "For a model with 18 classes this is quite ok and a rather small error.\n",
    "Quadratic Weighted Kappa measures the agreement between predictions and truth and penalizes big mistakes quadratically. This means if the prediction is very wrong \n",
    "it is weighted higher than if it is only slightly wrong. For our model qwk is 0.86 which is very good, 1 would be perfect. \n",
    "\n",
    "f1: 0.4471\n",
    "accuracy: 0.4553\n",
    "precision: 0.4345\n",
    "recall: 0.4740\n",
    "mae: 1.1529\n",
    "qwk: 0.8622\n",
    "plusMinus1: 0.7191\n",
    "\n",
    "Makwana, R., Bhatt, D., Delwadia, K. et al. Understanding and Attaining an Investment Grade Rating in the Age of Explainable AI. Comput Econ 66, 105126 (2025). https://doi.org/10.1007/s10614-024-10700-7\n",
    "showed with the same dataset that the could reach a accuracy of 0.80. That is higher than our accuracy. The difference from their approach to ours\n",
    "is that they only used 9 features instead of the 18 features we used. This is most likely the reason for the difference in performance.\n",
    "\n",
    "The performance of a random classifier would be 1/18 because we have 18 classes. This results in 0,055 so 5,5% would be correctly classified. Our model is much better\n",
    "\n",
    "The success criteria we defined in the beginning: \n",
    "-) Achieve a macro-averaged F1 score of at least 0.7 on the test set.\n",
    "-) Achieve at least a recall of 0.75 for the lowest rating classes.\n",
    "-) Keep the difference between validation and test F1 below 0.05.\n",
    "-) Maintain accuracy for all classes rated above 0.60 where sample size is sufficient.\n",
    "The only criteria where we are successful is 'Keep the difference between validation and test F1 below 0.05.' F1 for validation set is 0.45 and for test 0.4471\n",
    "so not even 1 percent difference. The other criteria are not fulfilled for reasons talked about before. \n",
    "\"\"\"\n",
    "bias_comment = \"\"\" \n",
    "We have not protected attribute like gender, race, age in our dataset. So we will use the attribute 'sic_code' to test if there is a bias towards. \n",
    "The sic_code is a code which shows what main business are a company has. \n",
    "\n",
    "The results show that\n",
    "model performance varies between industries. Some SIC groups achieve\n",
    "higher accuracy and lower mean absolute error, while others show larger\n",
    "prediction errors.\n",
    "\n",
    "The directional bias is close to zero for most SIC groups, indicating\n",
    "that the model does not systematically over- or under-rate companies.\n",
    "One group shows a large directional bias, but this group contains only\n",
    "very few samples and is therefore not reliable. Overall, the observed\n",
    "differences indicate performance similar across industries rather\n",
    "than systematic discriminatory bias.\n",
    "\n",
    "SIC_Group  Samples  Accuracy       MAE  PlusMinus1  DirectionalBias\n",
    "6          7       98  0.540816  0.857143    0.765306        -0.122449\n",
    "5          6       35  0.571429  0.914286    0.800000        -0.114286\n",
    "1          2      235  0.451064  1.097872    0.723404        -0.059574\n",
    "2          3      275  0.472727  1.127273    0.749091        -0.283636\n",
    "4          5      102  0.421569  1.147059    0.686275        -0.303922\n",
    "3          4      236  0.449153  1.271186    0.728814         0.050847\n",
    "0          1      145  0.420690  1.282759    0.634483         0.027586\n",
    "7          8       33  0.272727  1.303030    0.666667        -0.212121\n",
    "8          9        5  0.400000  2.400000    0.400000         1.600000\n",
    "\"\"\"\n",
    "\n",
    "evaluate_activity = [\n",
    "    f':evaluate_final_model rdf:type prov:Activity .',\n",
    "    f':evaluate_final_model sc:isPartOf :evaluation_phase .',\n",
    "    f':evaluate_final_model rdfs:label \"Final Model Evaluation on Test Set\" .',\n",
    "    f':evaluate_final_model rdfs:comment \"\"\"{eval_comment}\"\"\" .',\n",
    "    f':evaluate_final_model prov:startedAtTime \"{start_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:endedAtTime \"{end_time_eval}\"^^xsd:dateTime .',\n",
    "    f':evaluate_final_model prov:qualifiedAssociation :{eval_ass_uuid} .',\n",
    "    \n",
    "    f':{eval_ass_uuid} prov:agent :{eval_code_writer} .',\n",
    "    f':{eval_ass_uuid} rdf:type prov:Association .',\n",
    "    f':{eval_ass_uuid} prov:hadRole :{code_writer_role} .',\n",
    "\n",
    "    # Inputs\n",
    "    f':evaluate_final_model prov:used {final_model} .',\n",
    "    f':evaluate_final_model prov:used {test_set} .',\n",
    "    \n",
    "    # Reference to Data Mining Success Criteria from Phase 1\n",
    "    f':evaluate_final_model prov:used :bu_data_mining_success_criteria .',\n",
    "\n",
    "    # Document you final model performance\n",
    "    f':model_performance_result rdf:type mls:ModelEvaluation .',\n",
    "    f':model_performance_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':model_performance_result rdfs:label \"Model performance\" .',\n",
    "    f':model_performance_result rdfs:comment {performance_comment} .',\n",
    "    # Hint: you evaluate bias in this way:\n",
    "    f':bias_evaluation_result rdf:type mls:ModelEvaluation .',\n",
    "    f':bias_evaluation_result prov:wasGeneratedBy :evaluate_final_model .',\n",
    "    f':bias_evaluation_result rdfs:label \"Bias Analysis\" .',\n",
    "    f':bias_evaluation_result rdfs:comment {bias_comment} .',\n",
    "]\n",
    "engine.insert(evaluate_activity, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785c94b",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Activity that follows is part of the Deployment Phase\n",
    "\n",
    "deployment_phase_executor = [\n",
    "f':deployment_phase rdf:type prov:Activity .',\n",
    "f':deployment_phase rdfs:label \"Deployment Phase\" .', \n",
    "]\n",
    "engine.insert(deployment_phase_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176313c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Documentation\n",
    "#############################################\n",
    "\n",
    "comparison_and_recommendations_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "ethical_aspects_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "monitoring_plan_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "reproducibility_reflection_comment = \"\"\"\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "dep_ass_uuid_executor = \"72a921e0-1234-4567-89ab-cdef01234567\" # Generate once\n",
    "deployment_executor = [\n",
    "f':plan_deployment rdf:type prov:Activity .',\n",
    "f':plan_deployment sc:isPartOf :deployment_phase .', # Connect to Parent Phase\n",
    "f':plan_deployment rdfs:label \"Plan Deployment\"@en .',\n",
    "\n",
    "f':plan_deployment prov:qualifiedAssociation :{dep_ass_uuid_executor} .',\n",
    "f':{dep_ass_uuid_executor} prov:agent :{executed_by} .',\n",
    "f':{dep_ass_uuid_executor} rdf:type prov:Association .',\n",
    "f':{dep_ass_uuid_executor} prov:hadRole :{code_executor_role} .', \n",
    "]\n",
    "engine.insert(deployment_executor, prefixes=prefixes)\n",
    "\n",
    "\n",
    "deployment_data_executor = [\n",
    "#6a\n",
    "f':dep_recommendations rdf:type prov:Entity .',\n",
    "f':dep_recommendations prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_recommendations rdfs:label \"6a Business Objectives Reflection and Deployment Recommendations\" .',\n",
    "f':dep_recommendations rdfs:comment \"\"\"{comparison_and_recommendations_comment}\"\"\" .',\n",
    "#6b\n",
    "f':dep_ethical_risks rdf:type prov:Entity .',\n",
    "f':dep_ethical_risks prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_ethical_risks rdfs:label \"6b Ethical Aspects and Risks\" .',\n",
    "f':dep_ethical_risks rdfs:comment \"\"\"{ethical_aspects_comment}\"\"\" .',\n",
    "#6c\n",
    "f':dep_monitoring_plan rdf:type prov:Entity .',\n",
    "f':dep_monitoring_plan prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_monitoring_plan rdfs:label \"6c Monitoring Plan\" .',\n",
    "f':dep_monitoring_plan rdfs:comment \"\"\"{monitoring_plan_comment}\"\"\" .',\n",
    "#6d\n",
    "f':dep_reproducibility_reflection rdf:type prov:Entity .',\n",
    "f':dep_reproducibility_reflection prov:wasGeneratedBy :plan_deployment .',\n",
    "f':dep_reproducibility_reflection rdfs:label \"6d Reproducibility Reflection\" .',\n",
    "f':dep_reproducibility_reflection rdfs:comment \"\"\"{reproducibility_reflection_comment}\"\"\" .',\n",
    "\n",
    "]\n",
    "engine.insert(deployment_data_executor, prefixes=prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528dac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70d410af",
   "metadata": {},
   "source": [
    "# Generate Latex Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f44e16",
   "metadata": {},
   "source": [
    "The following cells give you an example of how to automatically create a Latex Report from your provenance documentation.\n",
    "\n",
    "Feel free to use the example provided. If you use it, you should adapt and extend it with relevant sections/tables/plots/... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37046b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_iri = f\"https://starvers.ec.tuwien.ac.at/BI2025/{group_id}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d887eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes cleaning functions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def latex_escape(text: str | None) -> str:\n",
    "    if text is None: return \"\"\n",
    "    text = str(text)\n",
    "    text = text.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "    pairs = [\n",
    "        (\"&\", r\"\\&\"), (\"%\", r\"\\%\"), (\"$\", r\"\\$\"), (\"#\", r\"\\#\"), \n",
    "        (\"_\", r\"\\_\"), (\"{\", r\"\\{\"), (\"}\", r\"\\}\"), \n",
    "        (\"~\", r\"\\textasciitilde{}\"), (\"^\", r\"\\textasciicircum{}\")\n",
    "    ]\n",
    "    for k, v in pairs:\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_rdf(x) -> str:\n",
    "    if hasattr(x, \"toPython\"): return str(x.toPython())\n",
    "    if x is None: return \"\"\n",
    "    s = str(x).strip()\n",
    "    s = s.strip('\"').strip(\"'\")\n",
    "    s = s.strip()\n",
    "    if \"^^\" in s:\n",
    "        s = s.split(\"^^\")[0].strip('\"')\n",
    "        \n",
    "    return s\n",
    "\n",
    "def fmt_iso(ts: str) -> str:\n",
    "    if not ts: return \"\"\n",
    "    try:\n",
    "        clean_ts = ts.split(\"^^\")[0].strip('\"')\n",
    "        clean_ts = clean_ts.replace(\"Z\", \"+00:00\") if clean_ts.endswith(\"Z\") else clean_ts\n",
    "        return datetime.fromisoformat(clean_ts).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return latex_escape(str(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d948da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell includes exemplary queries for different phases\n",
    "\n",
    "\n",
    "### Author Block\n",
    "author_query = f\"\"\"\n",
    "{prefix_header}\n",
    "PREFIX iao: <http://purl.obolibrary.org/obo/>\n",
    "\n",
    "SELECT DISTINCT ?uri ?given ?family ?matr WHERE {{\n",
    "  VALUES ?uri {{ :{student_a} :{student_b} }}\n",
    "  \n",
    "  ?uri a foaf:Person .\n",
    "  ?uri foaf:givenName ?given .\n",
    "  ?uri foaf:familyName ?family .\n",
    "  ?uri iao:IAO_0000219 ?matr .\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "res_authors = engine.query(author_query)\n",
    "author_block_latex = \"\"\n",
    "\n",
    "if not res_authors.empty: # type:ignore\n",
    "    for _, row in res_authors.iterrows(): # type:ignore\n",
    "\n",
    "        uri_str = str(row['uri'])\n",
    "        given = latex_escape(clean_rdf(row['given']))\n",
    "        family = latex_escape(clean_rdf(row['family']))\n",
    "        matr = latex_escape(clean_rdf(row['matr']))\n",
    "        if student_a in uri_str:\n",
    "            responsibility = \"Student A\"\n",
    "        elif student_b in uri_str:\n",
    "            responsibility = \"Student B\"\n",
    "        else:\n",
    "            responsibility = \"Student\"\n",
    "        \n",
    "        author_block_latex += rf\"\"\"\n",
    "          \\author{{{given} {family}}}\n",
    "          \\authornote{{{responsibility}, Matr.Nr.: {matr}}}\n",
    "          \\affiliation{{\n",
    "            \\institution{{TU Wien}}\n",
    "            \\country{{Austria}}\n",
    "          }}\n",
    "          \"\"\"\n",
    "\n",
    "### Business Understanding example\n",
    "bu_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?ds_comment ?bo_comment WHERE {{\n",
    "  OPTIONAL {{ :bu_data_source_and_scenario rdfs:comment ?ds_comment . }}\n",
    "  OPTIONAL {{ :bu_business_objectives rdfs:comment ?bo_comment . }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_bu = engine.query(bu_query)\n",
    "row_bu = res_bu.iloc[0] if not res_bu.empty else {} # type:ignore\n",
    "bu_data_source = latex_escape(clean_rdf(row_bu.get(\"ds_comment\", \"\")))\n",
    "bu_objectives  = latex_escape(clean_rdf(row_bu.get(\"bo_comment\", \"\")))\n",
    "\n",
    "\n",
    "### Data Understanding examples\n",
    "# Example Dataset Description\n",
    "du_desc_query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?desc WHERE {{ :raw_data sc:description ?desc . }} LIMIT 1\n",
    "\"\"\"\n",
    "res_du_desc = engine.query(du_desc_query)\n",
    "row_du_desc = res_du_desc.iloc[0] if not res_du_desc.empty else {} # type:ignore\n",
    "du_description = latex_escape(clean_rdf(row_du_desc.get(\"desc\", \"\")))\n",
    "\n",
    "# Example Feature Columns Table\n",
    "du_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?name (SAMPLE(?dtypeRaw) as ?dtype) (SAMPLE(?descRaw) as ?desc) WHERE {{\n",
    "  :raw_data cr:recordSet ?rs .\n",
    "  ?rs cr:field ?field .\n",
    "  ?field sc:name ?name .\n",
    "  ?field sc:description ?descRaw .\n",
    "  ?field cr:dataType ?dtypeRaw .\n",
    "}} \n",
    "GROUP BY ?name\n",
    "ORDER BY ?name\n",
    "\"\"\"\n",
    "res_du = engine.query(du_query)\n",
    "du_rows = []\n",
    "if not res_du.empty: # type:ignore\n",
    "    for _, f in res_du.iterrows(): # type:ignore\n",
    "        dtype_raw = clean_rdf(f.get(\"dtype\", \"\"))\n",
    "        if '#' in dtype_raw: dtype = dtype_raw.split('#')[-1]\n",
    "        elif '/' in dtype_raw: dtype = dtype_raw.split('/')[-1]\n",
    "        else: dtype = dtype_raw\n",
    "        \n",
    "        desc = clean_rdf(f.get(\"desc\", \"\"))\n",
    "        row_str = f\"{latex_escape(clean_rdf(f['name']))} & {latex_escape(dtype)} & {latex_escape(desc)} \\\\\\\\\"\n",
    "        du_rows.append(row_str)\n",
    "du_table_rows = \"\\n    \".join(du_rows)\n",
    "\n",
    "### Modeling example\n",
    "# Hyperparameters\n",
    "hp_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?hpName (SAMPLE(?hpValRaw) as ?hpVal) (MAX(?hpDescRaw) as ?hpDesc) WHERE {{\n",
    "  ?run sc:isPartOf :train_and_finetune_model .\n",
    "  ?run mls:hasInput ?setting .\n",
    "  ?setting a mls:HyperParameterSetting .\n",
    "  ?setting mls:hasValue ?hpValRaw .\n",
    "  ?setting mls:specifiedBy ?hpDef .\n",
    "  ?hpDef rdfs:label ?hpName .\n",
    "  OPTIONAL {{ ?hpDef rdfs:comment ?hpDescRaw . }}\n",
    "}} \n",
    "GROUP BY ?hpName\n",
    "ORDER BY ?hpName\n",
    "\"\"\"\n",
    "res_hp = engine.query(hp_query)\n",
    "hp_rows = []\n",
    "if not res_hp.empty: #type:ignore\n",
    "    for _, row in res_hp.iterrows(): #type:ignore\n",
    "        name = latex_escape(clean_rdf(row['hpName']))\n",
    "        val  = latex_escape(clean_rdf(row['hpVal']))\n",
    "        desc = latex_escape(clean_rdf(row.get('hpDesc', '')))\n",
    "        hp_rows.append(rf\"{name} & {desc} & {val} \\\\\")\n",
    "\n",
    "hp_table_rows = \"\\n    \".join(hp_rows)\n",
    "\n",
    "# Run Info\n",
    "run_query = f\"\"\"\n",
    "{prefix_header}\n",
    "\n",
    "SELECT ?algoLabel ?start ?end ?metricLabel ?metricVal WHERE {{\n",
    "  OPTIONAL {{ :train_and_finetune_model prov:startedAtTime ?start ; prov:endedAtTime ?end . }}\n",
    "  OPTIONAL {{\n",
    "      ?run sc:isPartOf :train_and_finetune_model .\n",
    "      ?run mls:realizes ?algo .\n",
    "      ?algo rdfs:label ?algoLabel .\n",
    "  }}\n",
    "  OPTIONAL {{\n",
    "    ?run sc:isPartOf :train_and_finetune_model .\n",
    "    ?run mls:hasOutput ?eval .\n",
    "    ?eval a mls:ModelEvaluation ; mls:hasValue ?metricVal .\n",
    "    OPTIONAL {{ ?eval mls:specifiedBy ?m . ?m rdfs:label ?metricLabel . }}\n",
    "  }}\n",
    "}} LIMIT 1\n",
    "\"\"\"\n",
    "res_run = engine.query(run_query)\n",
    "row_run = res_run.iloc[0] if not res_run.empty else {} #type:ignore\n",
    "mod_algo  = latex_escape(clean_rdf(row_run.get(\"algoLabel\", \"\")))\n",
    "mod_start = latex_escape(fmt_iso(clean_rdf(row_run.get(\"start\"))))\n",
    "mod_end   = latex_escape(fmt_iso(clean_rdf(row_run.get(\"end\"))))\n",
    "mod_m_lbl = latex_escape(clean_rdf(row_run.get(\"metricLabel\", \"\")))\n",
    "raw_val = clean_rdf(row_run.get('metricVal', ''))\n",
    "mod_m_val = f\"{float(raw_val):.4f}\" if raw_val else \"\"\n",
    "\n",
    "print(\"Data extraction done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8fa1c",
   "metadata": {},
   "source": [
    "The following includes the Latex report itself. It fills in the query-results from the cell before. The ACM Template is already filled. \n",
    "Make sure that you update Student A and B accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ce52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_content = rf\"\"\"\\documentclass[sigconf]{{acmart}}\n",
    "\n",
    "\\AtBeginDocument{{ \\providecommand\\BibTeX{{ Bib\\TeX }} }}\n",
    "\\setcopyright{{acmlicensed}}\n",
    "\\copyrightyear{{2025}}\n",
    "\\acmYear{{2025}}\n",
    "\\acmDOI{{XXXXXXX.XXXXXXX}}\n",
    "\n",
    "\\acmConference[BI 2025]{{Business Intelligence}}{{-}}{{-}}\n",
    "\n",
    "\\begin{{document}}\n",
    "\n",
    "\\title{{BI2025 Experiment Report - Group {group_id}}}\n",
    "%% ---Authors: Dynamically added ---\n",
    "{author_block_latex}\n",
    "\n",
    "\\begin{{abstract}}\n",
    "  This report documents the machine learning experiment for Group {group_id}, following the CRISP-DM process model.\n",
    "\\end{{abstract}}\n",
    "\n",
    "\\ccsdesc[500]{{Computing methodologies~Machine learning}}\n",
    "\\keywords{{CRISP-DM, Provenance, Knowledge Graph, Machine Learning}}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "%% --- 1. Business Understanding ---\n",
    "\\section{{Business Understanding}}\n",
    "\n",
    "\\subsection{{Data Source and Scenario}}\n",
    "{bu_data_source}\n",
    "\n",
    "\\subsection{{Business Objectives}}\n",
    "{bu_objectives}\n",
    "\n",
    "%% --- 2. Data Understanding ---\n",
    "\\section{{Data Understanding}}\n",
    "\\textbf{{Dataset Description:}} {du_description}\n",
    "\n",
    "The following features were identified in the dataset:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Raw Data Features}}\n",
    "  \\label{{tab:features}}\n",
    "  \\begin{{tabular}}{{lp{{0.2\\linewidth}}p{{0.4\\linewidth}}}}\n",
    "    \\toprule\n",
    "    \\textbf{{Feature Name}} & \\textbf{{Data Type}} & \\textbf{{Description}} \\\\\n",
    "    \\midrule\n",
    "    {du_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "%% --- 3. Data Preparation ---\n",
    "\\section{{Data Preparation}}\n",
    "\\subsection{{Data Cleaning}}\n",
    "Describe your Data preparation steps here and include respective graph data.\n",
    "\n",
    "\n",
    "%% --- 4. Modeling ---\n",
    "\\section{{Modeling}}\n",
    "\n",
    "\\subsection{{Hyperparameter Configuration}}\n",
    "The model was trained using the following hyperparameter settings:\n",
    "\n",
    "\\begin{{table}}[h]\n",
    "  \\caption{{Hyperparameter Settings}}\n",
    "  \\label{{tab:hyperparams}}\n",
    "  \\begin{{tabular}}{{lp{{0.4\\linewidth}}l}}\n",
    "    \\toprule\n",
    "    \\textbf{{Parameter}} & \\textbf{{Description}} & \\textbf{{Value}} \\\\\n",
    "    \\midrule\n",
    "    {hp_table_rows}\n",
    "    \\bottomrule\n",
    "  \\end{{tabular}}\n",
    "\\end{{table}}\n",
    "\n",
    "\\subsection{{Training Run}}\n",
    "A training run was executed with the following characteristics:\n",
    "\\begin{{itemize}}\n",
    "    \\item \\textbf{{Algorithm:}} {mod_algo}\n",
    "    \\item \\textbf{{Start Time:}} {mod_start}\n",
    "    \\item \\textbf{{End Time:}} {mod_end}\n",
    "    \\item \\textbf{{Result:}} {mod_m_lbl} = {mod_m_val}\n",
    "\\end{{itemize}}\n",
    "\n",
    "%% --- 5. Evaluation ---\n",
    "\\section{{Evaluation}}\n",
    "\n",
    "%% --- 6. Deployment ---\n",
    "\\section{{Deployment}}\n",
    "\n",
    "\\section{{Conclusion}}\n",
    "\n",
    "\\end{{document}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c947b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell stores the Latex report to the data/report directory\n",
    "\n",
    "out_dir = os.path.join(\"data\", \"report\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, \"experiment_report.tex\")\n",
    "\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex_content)\n",
    "\n",
    "print(f\"Report written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39428c21-a434-4905-bb07-85cfc135eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "{prefix_header}\n",
    "SELECT ?s ?p ?o\n",
    "WHERE {{\n",
    "  ?s ?p ?o .\n",
    "}}\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "res = engine.query(query)\n",
    "print(res)  # zeigt eine schne Tabelle in Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848b206-4c6d-4a8d-aa97-5ef407563fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
